---
title: "STA13824 - Análise de regressão"
subtitle: "Violação de suposições: Heteroscedasticidade e Autocorrelação"
author: "<br/><br/>"
institute: "LECON/DEST - UFES"
date: "Vitória, ES. - `r format(Sys.Date(), format='%d/%m/%Y')`"
output:
  xaringan::moon_reader:
    includes:
      after_body: ["insert-logo.html"]
    css: [xaringan-themer.css, "https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.7.0/animate.min.css"]
    lib_dir: libs
    nature:
      ratio: '16:9' 
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

class: animated, fadeIn
```{r xaringan-themer, include=FALSE, warning=FALSE}
library("xaringanthemer"); library("dplyr"); library("DT"); library("plotly")
style_mono_light(base_color = "#23395b")
```
```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```
```{r, load_refs, include=FALSE, cache=FALSE}
library("RefManageR"); library("bibtex")
BibOptions(check.entries = FALSE,
           bib.style = "authoryear",
           cite.style = "authoryear",
           style = "markdown",
           hyperlink = FALSE,
           dashed = FALSE)
myBib <- ReadBib("ref_ementa.bib", check = FALSE)
```

<style> body {text-align: justify} </style> <!-- Justify text. -->

### A4. Esfericidade dos erros

De acordo com **A4.**, a variância dos erros é tal que 

$$\mathbb{E}[\mathbf{\epsilon\epsilon}'|\mathbf{X}]=\sigma^2\mathbf{I},$$
i.e., a matriz de covariâncias do termo de erro é uma matriz diagonal. Em outras
palavras, as variâncias dos elementos do vetor $\epsilon$ são todas iguais e os
mesmo são não-correlacionados. Dessa forma, temos que

<div class="math">
$$
  \begin{aligned}
    \textrm{cov}\left[\widehat{\beta}\,\big|\,\mathbf{X}\right]&=\sigma^2\left(\mathbf{X}'\mathbf{X}\right)^{-1}.
  \end{aligned}
$$
</div>


#### Considerações

1. E se as variâncias dos elementos do vetor $\epsilon$ são diferentes, i.e.,
o termo de erro é heterogêneo?

--

1. E se os elementos fora da diagonal da matriz 
$\mathbb{E}[\mathbf{\epsilon\epsilon}'|\mathbf{X}]=\sigma^2\mathbf{I}$
são diferentes de zero, i.e., existe uma estrutura de dependência entre os
elementos do vetor $\epsilon$?

<br></br>

Para detalhes `r Citet(myBib, "montgomery:peck:2007")`, `r Citet(myBib, "greene:2007")` e `r Citet(myBib, "gujarati:porter:gunasekar:2017")`.


---
class: animated, slideInRight

### Heteroscedaticidade - I

A **Heteroscedasticidade** refere-se apenas à diferença nas variâncias dos elementos
do termo de erro. Entre outras razões, a heteroscedasticidade pode ser causada por:

* **Dados atípicos**: A presença de dados atípicos no conjunto de observações
causa efeitos significativos na variabilidade do conjunto;

* **Assimetria na distribuição das covariáveis**: dependendo do grau de assimetria,
a evidencia pode ser maior;

* **Transformações incorretas**: a heterocedasticidade também pode surgir pelo uso
de transformações incorretas nos dados ou por causa da forma funcional incorreta 
do modelo, por exemplo, o uso de modelos lineares ao invés de modelos log-lineares
e viceversa.

<br></br>

Sugestão de leitura,  Goldberger, A. S. (1964). Econometric Theory. New York: John Wiley & Sons. pp. 238–243.


---
class: animated, slideInRight

### Heteroscedaticidade - II

#### Estimadores de MQO

A solução dos sistema de equações normais é dada por

<div class="math">
$$
  \begin{aligned}
\mathbf{\widehat{\beta}}&=\mathbf{\beta}+\left(\mathbf{X}'\mathbf{X}\right)^{-1}\mathbf{X}'\mathbf{\epsilon}.
  \end{aligned}
$$
</div>

Daí, pela Suposição **A3.**, i.e., $\mathbb{E}[\mathbf{\epsilon}|\mathbf{X}]=\mathbf{0}$,
temos que:

$$\mathbb{E}\left[\widehat{\mathbf{\beta}}\,\big|\,\mathbf{X}\right]=\mathbf{\beta}+\mathbb{E}\left[\left(\mathbf{X}'\mathbf{X}\right)^{-1}\mathbf{X}'\mathbf{\epsilon}\,\big|\,\mathbf{X}\right]=\mathbf{\beta}+\left(\mathbf{X}'\mathbf{X}\right)^{-1}\mathbf{X}'\mathbb{E}\left[\mathbf{\epsilon}\,\big|\,\mathbf{X}\right]=\mathbf{\beta}.$$

> Pelas Suposições **A1-A3**, o estimador MQO é não-viesado para $\mathbf{\beta}$.

Mesmo se houver heteroscedasticidade o estimador de MQO é **consistente**. No 
cenário heteroscedástico, podemos supor que $\mathbb{E}\left[\mathbf{\epsilon}\mathbf{\epsilon}'\,\big|\,\mathbf{X}\right]=\sigma^2\Omega$.
Dessa forma, a covariância do estimador é dada por

<div class="math">
$$
  \begin{aligned}
\textrm{cov}\left[\widehat{\beta}\,\big|\,\mathbf{X}\right]
&=\left(\mathbf{X}'\mathbf{X}\right)^{-1}\mathbf{X}'\mathbb{E}\left[\mathbf{\epsilon}\mathbf{\epsilon}'\,\big|\,\mathbf{X}\right]\mathbf{X}\left(\mathbf{X}'\mathbf{X}\right)^{-1}
=\sigma^2\left(\mathbf{X}'\mathbf{X}\right)^{-1}\mathbf{X}'\Omega\mathbf{X}\left(\mathbf{X}'\mathbf{X}\right)^{-1},
  \end{aligned}
$$
</div>

diferente de $\sigma^2\left(\mathbf{X}'\mathbf{X}\right)^{-1}$. Dessa forma,
o Teorema de Gauss-Markov não é mais válido.


---
class: animated, slideInRight

### Alguns exemplos - I

O vetor de erros será heteroscedástico se a sua variância depende de $i$, ou seja, se $\sigma^2=\sigma^2_i$, com $i=1,2,3,\ldots,n$. Se as variâncias $\sigma_i^2$ são 
conhecidas, pode-se obtar pela seguinte transformção

$$\frac{Y_i}{\sigma_i}=\beta_0\frac{1}{\sigma_i}+\beta_1\frac{x_{1i}}{\sigma_i}+\cdots+\beta_k\frac{x_{ki}}{\sigma_i}+\frac{\epsilon_{i}}{\sigma_i},$$
ou

$$Y_i^*=\beta_0^*+\beta_1^*x_{1i}^*+\cdots+\beta_k^*x_{ki}^*+\epsilon_{i}^*.$$
Observe que,

$$\mathrm{var}\left[\epsilon_i^*\right]=\mathbb{E}\left[\frac{\epsilon_i}{\sigma_i}\right]^2=\frac{1}{\sigma_i^2}\mathbb{E}\left[\epsilon_i^2\right]=\frac{1}{\sigma^2_i}\sigma^2_i=1.$$
A variância do termo de erro transformado é constante.


---
class: animated, slideInRight

### Alguns exemplos - II

Outros cenários plausíveis temos quando 

* A variância do erro é proporcional a uma das covariáveis, i.e., $\sigma^2_i=\sigma^2 x_i$.
Nesse caso, pondere com $\frac{1}{\sqrt{x_i}}$;

* $\sigma^2_i$ é proporcional ao quadrado de uma das covariáveis, i.e., $\sigma^2_i=\sigma^2 x_i^2$.
Nesse caso, pondere com $\frac{1}{x_i}$;

* $\sigma^2_i$ é proporcional ao quadrado do valor médio de $Y$, i.e., $\sigma^2_i=\sigma^2\left(\mathbb{E}\left[Y_i\right]\right)^2$. Nesse caso, pondere com
$\frac{1}{\mathbb{E}\left[Y_i\right]}$;

Aplicando as ponderações, o estimador de MQO para os dados transformados é dado por

<div class="math">
$$
  \begin{aligned}
\mathbf{\widehat{\beta}}&=\left(\mathbf{X^*}'\mathbf{X^*}\right)^{-1}\mathbf{X^*}'Y^*.
  \end{aligned}
$$
</div>

Dessa forma, se as Suposições **A1-A3** são satisfeitas, o Teorema de Gauss-Markov é 
válido.

> Para o caso em que $\mathrm{cov}[\epsilon|\mathbf{X}]=\sigma^2\Omega$, pode-se
estender o resultado de Gauss-Markov referente às propriedades do estimador de MQO
para $\beta$.


---
class: animated, slideInRight

### Teorema de Gauss-Markov-Aitken

Considere o modelo $Y=\mathbf{X}\beta+\epsilon$, onde 
$\mathbb{E}[\epsilon|\mathbf{X}]=0$ e $\mathrm{cov}[\epsilon|\mathbf{X}]=\sigma^2\Omega$.
O estimador de mínimos quadrados generalizados (MQG) do vetor $\beta$, i.e.,

$$\widehat{\beta}=\left(\mathbf{X}'\Omega^{-1}\mathbf{X}\right)^{-1}\mathbf{X}'\Omega^{-1}Y$$
é o melhor estimador linear não-viesado e de mínima variância para $\beta$. A
matriz de covariâncias de $\widehat{\beta}$ é dada por 

$$\mathrm{cov}\left[\widehat{\beta}\right]=\sigma^2\left(\mathbf{X}'\Omega^{-1}\mathbf{X}\right)^{-1}.$$
> Um estimador para $\sigma^2$ é dado por

$$s^2=\frac{\left(Y-\mathbf{X}\beta\right)'\Omega^{-1}\left(Y-\mathbf{X}\beta\right)}{n-p}.$$
Para detalhes, `r Citet(myBib, "rao:1999")`, `r Citet(myBib, "yan:su:2009")`.


---
class: animated, slideInRight

### Heteroscedaticidade - VI

#### Consequências

* Mesmo na presença de heterocedasticidade os estimadores por MQO são 
não-viesados e consistentes. Porém, esses estimadores não têm variância mínima
e não são mais eficientes, i.e., o teorema de Gauss-Markov não é mais válido;

* Desde que as variâncias dos elementos do vetor $\epsilon$ sejam conhecidas, os
estimadores fornecidos pelo método de mínimos quadrados ponderados serão os
melhores estimadores lineares não-viesados;

* Os resultados dos testes para verificar significância dos parâmetros são
comprometidos;

* Há vários testes disponíveis para diagnósticar a heteroscedasticidade, mas 
é difícil dizer com certeza qual deles funcionará em determinada situação;

* O problema de heteroscedasticidade é difícil de corrigir, dependerá muito do 
padrão da mesma. Se a amostra é grande, podem-se obter estimaivas dos erros 
padrão com base nos estimadores de MQO. Outra alternativa é transformar os dados 
originais de tal forma que, nos dados transformados, relaxar o problema de 
heterocedasticidade.

---
class: animated, slideInRight

### Heteroscedaticidade - V

#### Métodos para detecção - I

Como no caso da multicolinearidade, não temos regras estabelecidas para detectar
a presença da heteroscedasticidade. Por exemplo. nos Casos 1 e 2, citados 
anteriormente, requer-se o conhecimento da variância dos elementos do vetor 
$\epsilon$, o que torna mais difícil a detecção. Alguns procedimentos gráficos podem ajudar na sua identificação.

.pull-left[

- Diagrama de dispersão dos resíduos contra $\widehat{Y}$ ou as covariáveis;

- Diagrama de dispersão dos resíduos ao quadrado contra $\widehat{Y}$ ou as 
covariáveias;

]

.pull-right[
```{r, out.width='85%', fig.align='center', echo=FALSE}
knitr::include_graphics("images/resid_plots.gif")
```
]

---
class: animated, slideInRight

### Heteroscedaticidade - VI

#### Métodos para detecção - II

* **Teste de White**: Proposto por White (1980)<sup>1</sup>, o teste propõe uma 
forma geral para avaliar a heteroscedasticidade e não presume uma forma particular 
da mesma. Infelizmente, pouco pode ser dito sobre seu poder. Para amostras finitas
o teste apresenta propriedades insatisfatórias, a menos que haja muito poucas 
covariáveis. O teste de White na verdade é um teste baseado em multiplicadores de Lagrange
para amostras suficientemente grandes, e não depende da suposição de normalidade 
do vetor de erros.

  Para ilustrar o teste, suponha o modelo $\quad Y_i=\beta_0+\beta_1x_{1i}+\beta_2x_{2i}+\epsilon_i$, 
com $i=1,2,\ldots,n$.

  $i.$ Ajuste o modelo de regressão e calcule os resíduos, isto é $e_i=Y_i-\widehat{Y}_i$;
  
  $ii.$  Ajuste o modelo auxiliar dado por
  
  $$e^2_i=\beta_0+\beta_1x_{1i}+\beta_2x_{2i}+\beta_3x^2_{1i}+\beta_4x^2_{2i}+\beta_5x_{1i}x_{2i}+\epsilon_i$$
e calcule o coeficiente de determinação, $R^2$;

***
<font size="3"><sup>1</sup>White, H. (1980) *A Heteroskedasticity-Consistent Covariance Matrix Estimator and a Direct Test for Heteroskedasticity*. Econometrica, 48(4), 817-838.</font>


---
class: animated, slideInRight

### Heteroscedaticidade - VI

* **Teste de White**:

  $iii.$ Sob a hipótese nula de que não há heteroscedasticidade, pode-se mostrar 
  que o tamanho da amostra multiplicado pelo $R^2$ da regressão auxiliar segue 
  assintoticamente uma distribuição de qui-quadrado com graus de liberdade 
  iguais ao número de covariáveis (excluindo-se o termo constante) na regressão 
  auxiliar. Dessa forma, para o exemplo, sob $H_0$,  $n\,R^2$ segue uma 
  distribuição $\chi^2_{5}$.
  
  $iv.$ Se $n\,R^2>\chi^2_{\alpha,5}$ rejeita-se $H_0$, em favor de $H_1$, i.e.,
  existe evidência suficiente para afirmar que há heteroscedasticidade. Caso
  contrário, se $n\,R^2<\chi^2_{\alpha,5}$ implica que $\beta_1=\beta_2=\cdots=\beta_5=0$,
  ou seja não há heteroscedasticidade.


---
class: animated, slideInRight

### Heteroscedaticidade - VII

* **Testes de Park<sup>1</sup>, Glejser<sup>2</sup> e Breusch–Pagan-Godfrey<sup>3</sup>**: 
Os 3 testes são similares e assim como o teste de White, cada um deles é um teste baseado
em multiplicadores de Lagrange.

**Algumas considerações**
1. O teste de Park é um caso especial do teste geral proposto por A. C. Harvey, A. C. em “Estimating regression models with multiplicative Heteroscedasticity.” Econometrica, 1976. v. 44, n. 3, p. 461-465;

1. Park sugere que $\sigma^2_i$ seja uma função de uma das covariável, tal que 
  $\sigma^2_i=\sigma^2x^\beta_i\exp\{u_i\}.$  Dessa forma, $\log\sigma^2_i=\log\sigma^2+\beta\log x_i+u_i$.;

1. Devido à similaridade dos testes propostos por Breusch & Pagan (1979) e Godfrey (1978), esses testes são conhecidos como testes de Breusch-Pagan-Godfrey para heteroscedasticidade.

***

<font size="3"><sup>1</sup>Park, R. E. (1966) *Estimation with heteroscedastic error terms*. Econometrica, v. 34, n. 4, p. 888. </font>

<font size="3"><sup>2</sup>Glejser, H. (1969) *A new test for heterocedasticity*. Journal of the American Statistical Association, v. 64, p. 316-323. </font>

<font size="3"><sup>3</sup>Breusch, T.; Pagan, A. (1979) *A simple test for heteroscedasticidade and random coefficient variation*. Econometrica, v. 47, p. 1287-1294. </font>

<font size="3"><sup>3</sup>Godfrey, L. (1978) *Testing for multiplicative heteroscedasticity*. Journal
of Econometrics, v. 8, p. 227-236. </font>


---
class: animated, slideInRight

### Heteroscedaticidade - VIII

Para ilustrar os testes, suponha o modelo 

  $$Y_i=\beta_0+\beta_1x_{1i}+\beta_2x_{2i}+\cdots+\beta_kx_{ki}+\epsilon_i, \qquad i=1,2,\ldots,n.$$
  $i.$ Ajuste o modelo de regressão e calcule os resíduos, isto é $e_i=Y_i-\widehat{Y}_i$;
  
  $ii.$  Ajuste os seguintes modelos auxiliares e calcule os respectivos 
  coeficientes de determinação:
  
  - **Park (1966)**: $\log e^2_i=\alpha_0+\alpha_1\log z_{1i}+\alpha_2\log z_{2i}+\cdots+\alpha_p\log z_{pi}+\epsilon_i$;

  - **Glejser (1969)**: $e^2_i=\alpha_0+\alpha_1z_{1i}+\alpha_2 z_{2i}+\cdots+\alpha_pz_{pi}+\epsilon_i$;

  - **Breusch, Pagan & Godfrey (1978-79)**: $\widetilde{e}^2_i=\alpha_0+\alpha_1z_{1i}+\alpha_2 z_{2i}+\cdots+\alpha_pz_{pi}+\epsilon_i$, onde $\widetilde{e}^2_i=\frac{e^2_i}{\frac1n\sum_{i=1}^ne^2_i}.$

> As variáveis $z_{1i},z_{2i},\ldots,z_{pi}$ no modelo auxiliar pode ser consideradas 
como algumas (ou todas) as covariáveis do modelo, i.e., $x_{1i},x_{2i},\ldots,x_{ki}$.

---
class: animated, slideInRight

### Heteroscedaticidade - VIII

$iii.$ Sob a hipótese nula de que não há heterocedasticidade, a estatística 
$n\,R^2$ é *assintoticamente* qui-quadrado com graus de liberdade iguais ao 
número de regressores (não incluindo o termo constante) na regressão auxiliar;

$iv.$ Se $n\,R^2>\chi^2_{\alpha,\,p}$ rejeita-se $H_0$, em favor de $H_1$, i.e.,
existe evidência suficiente para afirmar que há heteroscedasticidade. Caso
contrário, se $n\,R^2<\chi^2_{\alpha,\,p}$ implica que $\alpha_1=\alpha_2=\cdots=\alpha_p=0$,
ou seja não há heteroscedasticidade.
  
> **Algumas considerações** 
  - Os testes Park, Glesjer e Breusch-Pagan-Godfrey exigem conhecimento sobre a 
origem da heterocedasticidade, ou seja, as variáveis $z_{1i}, z_{2i},\ldots, z_{pi}$ 
são conhecidas por serem "responsáveis" pela heterocedasticidade;
  - No teste de Park, o termo de erro na regressão auxiliar pode não satisfazer 
  as suposições do modelo de regressão não-linear clássico e pode ser 
  heterocedástico em si. 
  - No teste de Glejser, o termo de erro é diferente de zero, é correlacionado 
  e é ironicamente heterocedástico. 
  - No Teste de Breusch-Pagan-Godfrey, o termo de erro é bastante sensível à 
  suposição de normalidade em amostras finitas.

---
class: animated, slideInRight

### Heteroscedaticidade - IX

* **Teste de Goldfeld–Quandt<sup>1</sup>**: Proposto por Goldfeld & Quandt (1972),
o teste baseia-se na realização de um teste $F$ para igualdade de variâncias.
Se os elementos do vetor de erros são homocedásticos, então a variância dos 
resíduos de uma parte das observações da amostra deve ser igual à variância dos 
resíduos de outra parte das observações da amostra. 

  Para ilustrar o teste, suponha o modelo 

  $$Y_i=\beta_0+\beta_1x_{1i}+\beta_2x_{2i}+\cdots+\beta_kx_{ki}+\epsilon_i, \qquad i=1,2,\ldots,n.$$
  $i.$ Identifique uma variável à qual a variância do erro está relacionada. 
Para fins ilustrativos, suponha que seja $X_1$ a covariável que está relacionada
positivamente com $\sigma^2_i$;

  $ii.$ Ordene ou classifique as observações de acordo com os valores de $X_1$, 
começando com o menor valor de $X_1$;

  $iii.$ Omita $c$ observações centrais, onde $c$ é especificado *a priori*, e 
divida as $n - c$ observações restantes em dois grupos cada um de $\frac{n - c}{2}$ 
observações. 

***

<font size="3"><sup>1</sup>Goldfeld, Stephen M.; Quandt, Richard E. (1972) Nonlinear methods in econometrics. Amsterdã: North Holland Publishing Company, p. 93-94. </font>

---
class: animated, slideInRight

### Heteroscedaticidade - IX

$iv.$ Ajuste regressões separadas para as primeiras e as últimas $\frac{n - c}{2}$ 
observações, e calcule as respectivas soma dos quadrados dos residuos. Sejam
$SQ_{Res1}$ e $SQ_{Res2}$ as somas dos quadrados dos resíduos correspondentes
aos menores valores de $X_1$ (grupo com a menor variabilidade) e aos maiores 
valores de $X_1$ (o grupo com a maior variabilidade), respectivamente;

$v.$ Calcule a estatística $F$

$$F=\frac{SQ_{Res1}/gl}{SQ_{Res2}/gl},$$
onde $gl=\frac{n-c-2(k+1)}{2}$, $k$ é o número de parâmetros a serem estimados 
(incluindo o intercepto);

$vi.$ Supondo **A5**, e sob $H_0$ (Homoscedasticidade), então $F$ segue uma distribuição $F_{\alpha,gl,gl}$. Rejeita-se $H_0$ se $F>F_{\alpha,gl,gl}$.

> A escolha de $c$, na maior parte, é arbitrária. Alguns autores sugeresm considerar
um valor de $c$ entre $\frac16$ e $\frac13$ do total de observações.


---
class: animated, slideInRight

### Heteroscedaticidade - X

#### Outros testes de heteroscedasticidade

* Koenker, R. (1981). *A note on studentizing a test for heteroscedasticity*. Journal of Econometrics, 17(1), 107-112.

* Koenker R.; Bassett, G. (1982) *Robust tests for heteroscedastividy based on regression quantiles*. Econometrica. v. 50, p. 43-61.

* Harrison, M. J. and McCabe, B. P. (1979) *A test for heteroscedasticity based on ordinary least squares Residuals*. Journal of the American Statistical Association, v. 74, p. 494-499. 

* SZROETER, J. (1978) *A class of parametric tests for heteroscedasticity in linear econometric models*. Econometrica, v. 46, p. 1.311-1.327. 

* Evans, M. A. and King, M. L. (1988) *A further class of tests for heteroscedasticity*. Journal of Econometrics, v. 37, p. 265-276.

> Lyon, J., & Tsai, C. (1996). [A Comparison of Tests for Heteroscedasticity](https://sci-hub.se/https://www.jstor.org/stable/2988471). Journal of the Royal Statistical Society. Series D (The Statistician), 45(3), 337-349.

No `R` use os pacotes: [`car`](https://cran.r-project.org/web/packages/car/car.pdf), [`lmtest`](https://cran.r-project.org/web/packages/lmtest/lmtest.pdf), [`skedastic`](https://cran.r-project.org/web/packages/skedastic/skedastic.pdf), [`plm`](https://cran.r-project.org/web/packages/plm/plm.pdf).


---
class: inverse, hide-logo, middle, center

# Medidas corretivas

---
class: animated, fadeIn

### Estimação de matrizes de covariâncias - I

Considere o modelo
 
$$Y=\mathbf{X}\beta+\epsilon.$$

No cenário heteroscedástico, suponha que $\mathbb{E}\left[\mathbf{\epsilon}\mathbf{\epsilon}'\,\big|\,\mathbf{X}\right]=\sigma^2_i\mathbf{I}=\Omega$, com $i=1,2,3\ldots,n$. Dessa forma, a covariância do estimador é dada por

<div class="math">
$$
  \begin{aligned}
\textrm{cov}\left[\widehat{\beta}\,\big|\,\mathbf{X}\right]
&=\left(\mathbf{X}'\mathbf{X}\right)^{-1}\mathbf{X}'\Omega\mathbf{X}\left(\mathbf{X}'\mathbf{X}\right)^{-1}=\Psi.
  \end{aligned}
$$
</div>

--
<br></br>

.center[
### <span style="color:rgba(141, 0, 69, 1)">Objetivo:</span>  obter um estimador de consistente para $\Psi$ tanto sob homoscedasticidade quanto sob heteroscedasticidade de forma desconhecida.
]


---
class: animated, slideInRight

### Estimação de matrizes de covariâncias - II

* **Estimador $\textrm{HC}0$**: Proposto por White (1980)<sup>1</sup>, sugere o
seguinte estimador para $\Psi$

$$\widehat{\Psi}_0=\left(\mathbf{X}'\mathbf{X}\right)^{-1}\mathbf{X}'\widehat{\Omega}\mathbf{X}\left(\mathbf{X}'\mathbf{X}\right)^{-1},$$
onde $\widehat{\Omega}=\textrm{diag}\left(e^2_i\right)$.

> Este estimador é amplamente usado na literatura e é fácil para implementar. 
O estimador é consistente tanto na homoscedasticidade, quanto na 
heteroscedasticidade. No entanto, tende a ser viesado (_subestima as variâncias_) 
e não confiável na presença de pontos de alavancagem e, portanto, os testes 
quase-t associados tendem a ser liberais (_rejeitam $H_0$ quando a mesma é 
verdadeira com uma frequência superior ao nivel de significância $\alpha$._)

<br></br>

***

<font size="3"><sup>1</sup>White, H. (1980) A heteroskedasticity-consistent covariance matrix
estimator and a direct test for heteroskedasticity. Econometrica,
48(4), p. 817–838. </font>


---
class: animated, slideInRight

### Estimação de matrizes de covariâncias - II

**Teste quase-t**: Suponha que deseja-se testar a seguinte 
hipótese $H_0: \beta_i=\beta_i^{(0)}$ vs $H_1: \beta_i\ne\beta_i^{(0)}$, para
algum $i=1,2,\ldots, k$. Podemos usar a estatística 

$$\tau=\frac{\widehat{\beta}_i-\beta_i^{(0)}}{\widehat{\textrm{var}}\left[\widehat{\beta}_i\right]},$$ 
onde $\widehat{\textrm{var}}\left[\widehat{\beta}_i\right]$ é o $i$-ésimo elemento
diagonal da matriz $\widehat{\Psi}_0$.

White (1980) mostrou que $\tau\overset{\textrm{d}}{\longrightarrow}N(0,1)$ quando
$n\to\infty$. Dessa forma, use quantis de uma normal-padrão para avaliar o teste.
Assim, rejeita-se $H_0$ quando $|\tau|>z_{\alpha/2}$.

> **Obs:** Não foi necessária a suposição de normalidade no vertor de erro.


---
class: animated, slideInRight


### Estimação de matrizes de covariâncias - III

Para melhorar o estimador de White (1980), várias variantes do $\textrm{HC}0$ 
foram propostas na literatura.

* **Estimador $\textrm{HC}1$**: Proposto por Hinkley (1977)<sup>1</sup>, sugere o
seguinte estimador para $\Psi$

$$\widehat{\Psi}_1=\left(\mathbf{X}'\mathbf{X}\right)^{-1}\mathbf{X}'\widehat{\Omega}\mathbf{X}\left(\mathbf{X}'\mathbf{X}\right)^{-1},$$
onde $\widehat{\Omega}=\frac{n}{n-p}\textrm{diag}\left(e^2_i\right)$.

> O estimador propõe uma correção dos graus de liberdade. Embora o viés do 
estimador seja menor do que $\textrm{HC}0$, ele ainda existe.


<br></br>

***

<font size="3"><sup>1</sup>Hinkley, D.V. (1977) Jackknifing in unbalanced 
situations. Technometrics, 19(3), p. 285–292. </font>


---
class: animated, slideInRight

### Estimação de matrizes de covariâncias - IV

* **Estimador $\textrm{HC}2$**: Proposto por McKinnon & White (1985)<sup>1</sup>, 
sugerem o seguinte estimador para $\Psi$

$$\widehat{\Psi}_2=\left(\mathbf{X}'\mathbf{X}\right)^{-1}\mathbf{X}'\widehat{\Omega}\mathbf{X}\left(\mathbf{X}'\mathbf{X}\right)^{-1},$$
onde 

$$\widehat{\Omega}=\textrm{diag}\left(\frac{e^2_i}{(1-h_{ii})^{\delta_{i}}}\right),$$
com $h_{ii}$ o $i$-ésimo elemento diagonal da matriz $\mathbf{H}$ e $\delta_i=1$.

> O estimador leva em consideração os pontos de alavancagem. Embora seja
imparcial sob homocedasticidade, irá produzir viés devido aos pontos de alta 
alavancagem.

<br></br>

***

<font size="3"><sup>1</sup>McKinnon, J.G. and White, H. (1985) Some heteroskedasticity-consistent covariance matrix estimators with improved finite sample
properties. Journal of Econometrics, 29(3), p. 305–325. </font>


---
class: animated, slideInRight

### Estimação de matrizes de covariâncias - V

* **Estimador $\textrm{HC}3$**: Proposto por Davidson & McKinnon (1993)<sup>1</sup>, 
sugere a mesma forma funcional do estimador $\textrm{HC}2$, considerando $\delta_i=2$.

> A proposta é um ligeiro ajuste do estimador $\textrm{HC}2$, que desconta mais
os efeitos dos pontos de alavancagem. O $\textrm{HC}3$ apresenta melhor desempenho
em termos da distribuição assintótica de teste quase-t. Recomenda-se para cenários
com pequena amostra, no entanto, para grandes amostras os estimadores $\textrm{HC}0$,
$\textrm{HC}1$, $\textrm{HC}2$ e $\textrm{HC}3$ têm quase o mesmo comportamento. 
Na presença de pontos de alavancagem o estimador tende a ser mais tendencioso e, 
portanto, o os testes quase-t associados tendem a ser mais liberais.

* **Estimador $\textrm{HC}4$**: Proposto por Cribari-Neto (2004)<sup>2</sup>, 
sugere a mesma forma funcional do estimador $\textrm{HC}2$, considerando 
$\delta_i=\min\left\{\dfrac{h_{ii}}{\bar{h}},4\right\}$, onde 
$\bar{h}=\frac{1}{n}\sum_{i=1}^nh_{ii}=\frac{1}{n}\textrm{tr}(\mathbf{H})=\frac{1}{n}p$,
daí, $\delta_i=\min\left\{\frac{n\, h_{ii}}{p},4\right\}$.


***

<font size="3"><sup>1</sup>Davidson, R. and McKinnon, J. (1993) Estimation and Inference in
Econometrics. New York, Oxford University Press. </font>

<font size="3"><sup>2</sup>Cribari-Neto, F. (2004) Asymptotic inference under heteroskedasticity of unknown form. Computational Statistics & Data Analysis, 45(2), p. 215–233. </font>


---
class: animated, slideInRight

### Estimação de matrizes de covariâncias - VI

* **Estimador $\textrm{HC}4m$**: Proposto por Cribari-Neto & Silva (2011)<sup>1</sup>, 
sugere a mesma forma funcional do estimador $\textrm{HC}2$, considerando 
$\delta_i=\min\left\{\frac{n\,h_{ii}}{p},\gamma_1\right\}+\min\left\{\frac{n\,h_{ii}}{p},\gamma_2\right\}$, onde $\gamma_1=1$ e $\gamma_2=1.5$.

* **Estimador $\textrm{HC}5$**: Proposto por Cribari-Neto, Souza & Vasconcellos (2007)<sup>2</sup>, 
sugere o seguinte estimador para $\Psi$

  $$\widehat{\Psi}_5=\left(\mathbf{X}'\mathbf{X}\right)^{-1}\mathbf{X}'\widehat{\Omega}\mathbf{X}\left(\mathbf{X}'\mathbf{X}\right)^{-1},$$
onde $\widehat{\Omega}=\textrm{diag}\left(\dfrac{e^2_i}{\sqrt{(1-h_{ii})^{\delta_{i}}}}\right)$, onde
$\delta_i=\min\left\{\dfrac{n\,h_{ii}}{p},\max\left\{4,\dfrac{n\,k\,h_{\max}}{p}\right\}\right\}$, com
$k=0.7$ e $h_{\max}=\max\{h_{ii}, i=1,2,\ldots,n\}$.

***

<font size="3"><sup>1</sup>Cribari-Neto, F. and da Silva, W. B. (2011) A new heteroskedasticity-
consistent covariance matrix estimator for the linear regression
model. AStA Advances in Statistical Analysis, 95(2), p. 129–146.</font>

<font size="3"><sup>2</sup>Cribari-Neto, F., Souza, T. C. and Vasconcellos, K. L. (2007) Inference under heteroskedasticity and leveraged data. Communications in Statistics—Theory and Methods, 36(10), p. 1877–1888.</font>

---
class: animated, slideInRight

### Estimação de matrizes de covariâncias - VII

* **Estimador $\textrm{HC}5m$**: Proposto por Li, Zhang, Zhang & Wang (2017)<sup>1</sup>, 
sugere a mesma forma funcional do estimador $\textrm{HC}2$, considerando 

$$\delta_i=k_1\min\left\{\frac{n\,h_{ii}}{p},\gamma_1\right\}+k_2\min\left\{\frac{n\,h_{ii}}{p},\gamma_2\right\}+k_3\min\left\{\frac{n\,h_{ii}}{p},\max\left\{4,\frac{nkh_{\max}}{p}\right\}\right\},$$
onde $h_{\max}=\max\{h_{ii}, i=1,2,\ldots,n\}$, $\gamma_1=1$ e $\gamma_2=1.5$ e $k_i\ge0$. Os autores sugerem $k_1=1, k_2=0, k_3=1$.


* **Estimador $\textrm{HC}6$**: Proposto por Aftab & Chand (2018)<sup>2</sup>, 
sugere a mesma forma funcional do estimador $\textrm{HC}2$, considerando 
$\delta_i=\min\left\{\frac{n\, h_{ii}}{p},\sqrt{\frac{n\,h_{\max}}{2p}}\right\}$.

***
<font size="3"><sup>1</sup>Li, S., Zhang, N., Zhang, X. and Wang, G. (2017) A new
heteroskedasticity-consistent covariance matrix estimator and inference under heteroskedasticity. Journal of Statistical Computation and Simulation, 87(1), p. 198–210.</font>

<font size="3"><sup>2</sup>Aftab, N. and Chand, S. (2018) A simulation-based evidence on
the improved performance of a new modified leverage adjusted heteroskedastic consistent covariance matrix estimator in the linear regression model. Kuwait Journal of Science, 45(3), p. 29–38.</font>

---
class: animated, slideInRight

### Estimação de matrizes de covariâncias - VIII

* **Estimador $\textrm{HC}7$**: Proposto por Aftab & Chand (2016)<sup>1</sup>, 
sugere o seguinte estimador para $\Psi$

  $$\widehat{\Psi}_7=\left(\mathbf{X}'\mathbf{X}\right)^{-1}\mathbf{X}'\widehat{\Omega}\mathbf{X}\left(\mathbf{X}'\mathbf{X}\right)^{-1},$$
onde $\widehat{\Omega}=\textrm{diag}\left(\sqrt{d_{ii}e^2_i}\right)$, onde $d_{ii}=\dfrac{r^2_ih_{ii}}{p\,(1-h_{ii})}$ e $r^2_i=\dfrac{e^2_ i}{s^2(1-h_{ii})}$
são os [resíduos estudentizados internamente](https://ffajardo64.github.io/statistical_learning/STA13824/aula9.html#3).

> Outras variantes são apresentadas em Salem, M., Fattah, A. A. and Rady, E. H. (2019) [A New Heteroscedasticity Consistent Covariance Matrix Estimator and Inference Based on Robust Methods](https://scholar.cu.edu.eg/sites/default/files/elhoussainy/files/16ctn07-pp2687_0.pdf).
Journal of Computational and Theoretical Nanoscience, 16(7), p. 2687–2694.

<br></br>

***
<font size="3"><sup>1</sup>Aftab, N. and Chand, S. (2016) A new heteroskedastic consistent
covariance matrix estimator using deviance measure. Pakistan Journal of Statistics and Operation Research, 12(2), p. 235–244.</font>


---
class: inverse, hide-logo, middle, center

# Autocorrelação

---
class: animated, slideInRight

### Algumas definições - I

* **Processo estocástico**: Um processo estocástico $\{X_t, t\in \cal{T}\}$ é uma
familia de variáveis aleatórias no mesmo espaço de probabilidade. O conjunto 
$\cal{T}$ é geralmente considerado como o conjunto de números inteiros ou o
conjunto de números reais.

> Uma série temporal não é mais do que a realização de um processo estocástico.

* **Estacionariedade estrita:** Um processo estocástico se diz *estritamente*
estacionário se todas as distribuições unidimensionais são invariantes sob
translações do tempo, i.e.,

  $$F(x_{t_1+\tau},x_{t_2+\tau},\ldots,x_{t_n+\tau})=F(x_{t_1},x_{t_2},\ldots,x_{t_n}),$$
  para quaisquer $t_1,t_2,\ldots,t_n,\tau\in \cal{T}$. Dessa forma, a média e a 
variância são constantes, i.e.

  $$\mathbb{E}[X_t]=\mu \qquad \text{e} \qquad \textrm{var}[X_t]=\sigma^2,$$
para todo $t\in\cal{T}$.

  Da mesma forma, a **função de autocovariância** de um processo 
estritamente estacionário é dada por

  $$\gamma(\tau)=\textrm{cov}[X_t,X_{t+\tau}].$$
---
class: animated, slideInRight

### Algumas definições - II

* **Estacionariedade fraca:** Um processo estocástico se diz *fracamente*
estacionário *see*:

  - $\mathbb{E}[X_t]=\mu$, para todo $t\in \cal{T}$;
  - $\mathbb{E}[X^2_ t]< \infty$, para todo $t\in \cal{T}$;
  - $\textrm{cov}[X_{t_1},X_{t_2}]=\gamma(t_1,t_2)$ é uma função exclusiva de 
  $|t_1-t_2|$. De outra forma, $\gamma(\tau)=\textrm{cov}[X_t,X_{t+\tau}]$.

> Na literatura, muitos autores referem-se à estacionariedade fraca como
*estacionariedade de segunda ordem* ou simplesmente como *estacionariedade*.

* **Função de autocovariância**: Seja $\{X_t;\, t\in \mathbb{Z}\}$ um processo
estacionário, com média zero. A função de autocovariância do processo define-se 
como $\gamma(h)=\mathbb{E}[X_tX_{t+h}]$ e tal que
  - $\gamma(0)>0$;
  - $\gamma(-h)=\gamma(h)$;
  - $|\gamma(h)|\le\gamma(0)$, para todo $h$;
  - $\gamma(h)$ é não-negativa definida, i.e., para quaisquer números reais 
  $a_1,a_2,\ldots,a_n$, temos que
   
   $$\sum_{i=1}^n\sum_{j=1}^na_i\gamma(i-j)a_j\ge0.$$

---
class: animated, slideInRight

### Algumas definições - III

* **Função de autocovariância**: Seja $\{X_t;\, t\in \mathbb{Z}\}$ um processo
estacionário, com média zero. A função de autocorrelação define-se como

  $$\rho(h)=\frac{\gamma(h)}{\gamma(0)}, \quad h=0\pm1,\pm2,\ldots$$
* **Ruído branco**: Dizemos que $\{\varepsilon_t;\, t\in \mathbb{Z}\}$ é um
processo de _ruído branco_ se as variáveis aleatórias $\varepsilon_t$ são 
não-correlacionadas, i.e. 

<div class="math">
  \[
  \begin{aligned}
  \gamma(h)=\begin{cases}
  \sigma^2, & \text{ se }\,  h=0;\\
  0, & \text{ se }\,  h\ne0.
  \end{cases}
  \end{aligned}
  \]
  </div>


* **Processos autorregressivos**: Dizemos que $\{X_t,\,t\in \mathbb{Z}\}$ é um
processo autorregressivo de ordem $p$ se satisfaz a equação em diferenças

$$X_t = \phi_0+ \phi_1X_{t-1}+\phi_2X_{t-2}+\cdots+\phi_pX_{t-p}+\varepsilon_t,$$
onde $\{\varepsilon_t\}$ é um processo de ruído branco.


---
class: animated, slideInRight

### Processos AR(1)

Dizemos que $\{X_t,\,t\in \mathbb{Z}\}$ é um
processo autorregressivo de ordem $1$ se satisfaz a equação em diferenças

$$X_t =\phi_0+\phi_1 X_{t-1}+\varepsilon_t,$$
onde $\{\varepsilon_t\}$ é um processo de ruído branco com média $0$ e variância
$\sigma^2$. Denota-se como $\textrm{AR}(1)$.

> Um processo $\textrm{AR}(1)$ será considerado estacionário quando $|\phi_1|<1$.
Nesse caso, 
* $\mathbb{E}[X_t]=\dfrac{\phi_0}{1-\phi_1}$ e $\textrm{var}[X_t]=\dfrac{\sigma^2}{1-\phi_1^2}$;
* A função de autocovariância é dada por 
$$\gamma(h)=\dfrac{\sigma^2}{1-\phi_1^2}\phi^{|h|}, \quad h\in\mathbb{Z},$$
  assim a função de autocorrelação (FAC) é dada por
  $$\rho(h)=\phi_1^{|h|}, \quad h\in\mathbb{Z}.$$
  
---
class: animated, slideInRight

### Exemplo:  AR(1) com $\phi_1=0.3$
.pull-left[
```{r, echo=FALSE,fig.align='left',message=FALSE,warning=FALSE}
set.seed(6589897)
  y <- arima.sim(n = 200, list(ar = 0.3), innov=rnorm(200))
  par(bg=NA)
  plot.ts(y, ylab='',xlab="")
  text(20, 2.0, expression(Phi[1] == 0.3),cex=2)

```
]

.pull-right[
```{r, echo=FALSE,fig.align='right',message=FALSE,warning=FALSE}
set.seed(6589897)
  y <- arima.sim(n = 200, list(ar = 0.3), innov=rnorm(200))
  op <- par(no.readonly=TRUE,bg=NA)
  layout(matrix(c(1, 1, 2, 2), 2, 2, byrow=TRUE))
  acf(y, main='Autocorrelações', ylab='', xlab="Defasagens",
      ylim=c(-1, 1), ci.col = "black")
  pacf(y, main='Autocorrelações parciais', ylab='',xlab="Defasagens",
       ylim=c(-1, 1), ci.col = "black")

```
]


---
class: animated, slideInRight

### Exemplo:  AR(1) com $\phi_1=0.9$
.pull-left[
```{r, echo=FALSE,fig.align='left',message=FALSE,warning=FALSE}
set.seed(6589897)
  y <- arima.sim(n = 200, list(ar = 0.9), innov=rnorm(200))
  par(bg=NA)
  plot.ts(y, ylab='',xlab="")
  text(20, -3.0, expression(Phi[1] == 0.9),cex=2)

```
]

.pull-right[
```{r, echo=FALSE,fig.align='right',message=FALSE,warning=FALSE}
set.seed(6589897)
  y <- arima.sim(n = 200, list(ar = 0.9), innov=rnorm(200))
  op <- par(no.readonly=TRUE,bg=NA)
  layout(matrix(c(1, 1, 2, 2), 2, 2, byrow=TRUE))
  acf(y, main='Autocorrelações', ylab='', xlab="Defasagens",
      ylim=c(-1, 1), ci.col = "black")
  pacf(y, main='Autocorrelações parciais', ylab='',xlab="Defasagens",
       ylim=c(-1, 1), ci.col = "black")

```
]


---
class: animated, slideInRight

### Autocorrelação - I

O caso mais comum é o de autocorrelação de primeira ordem. Neste cenário, 
considere 

$$Y_t=\beta_0+\beta_1x_{t1}+\cdots+\beta_kx_{tk}+\varepsilon_t,$$
onde $\varepsilon_t=\rho\,\varepsilon_{t-1}+\epsilon_t$, para $t=1,2,3,\ldots,n$ 
e $\{\epsilon_t\}$ um processo de ruído branco.

Quando $|\rho|<1$, $\{\varepsilon_t\}$ é um processo estacionário com média $0$ e
variância $\sigma_\varepsilon^2=\dfrac{\sigma^2_\epsilon}{1-\rho^2}$.

#### Algumas considerações

* Observe que $\sigma_\epsilon^2\ge\sigma_\varepsilon^2$;
* $\textrm{cor}(\varepsilon_t,\varepsilon_{t-1})=\dfrac{\textrm{cov}(\varepsilon_t,\varepsilon_{t-1})}{\sqrt{\text{var}[\varepsilon_t]\textrm{var}[\varepsilon_{t-1}]}}=\dfrac{\rho\sigma^2_{\varepsilon}}{\sigma^2_\varepsilon}=\rho$. Ou seja, o parâmetro $\rho$ representa a correlação entre $\varepsilon_t$ e $\varepsilon_{t-1}$;

> Em geral, $\textrm{cor}(\varepsilon_t,\varepsilon_{t-k})=\rho^k$, para 
$k=0,1,2,3,\ldots$.

---
class: animated, slideInRight

### Autocorrelação - II

#### Estimador de MQO

Na presença de autocorrelação, a matriz de covariâncias do vetor de erros é dada por

<div class="math">
\[\mathbb{E}[\varepsilon\varepsilon']=\dfrac{\sigma^2_\epsilon}{1-\rho^2}
\left[
\begin{array}{ccccc}
1&\rho&\rho^2&\cdots&\rho^{n-1} \\
\rho&1&\rho&\cdots&\rho^{n-2} \\
\rho^2 &\rho&1&&\vdots& \\
\vdots&\vdots&\vdots&\ddots &\rho\\
\rho^{n-1}&\rho^{n-2}&\cdots&\rho&1 \\
\end{array}
\right].
\]
</div>

> Quando $\rho=0$,  temos que $\mathbb{E}[\varepsilon\varepsilon']=\sigma^2\mathbf{I}$.

Assim como no caso heteroscedástico, na presença de autocorrelação, o estimador 
de MQO é não-viesado e consistente, mas não é eficiente. Dessa forma, o Teorema 
de Gauss-Markov não é mais válido.


---
class: animated, fadeIn

### Exemplo - I

Considere o modelo

$$Y_t=\beta_0+\beta_1x_{t}+\varepsilon_t, \quad t=1,2,3,\ldots, n,$$
onde $\varepsilon_t=\rho\,\varepsilon_{t-1}+\epsilon_t$ e $|\rho|<1$.

Observe que,

<div class="math">
\[
  \begin{aligned}
    Y_t&=\beta_0+\beta_1x_{t}+\varepsilon_t=\beta_0+\beta_1x_{t}+\rho\varepsilon_{t-1}+\epsilon_t=\beta_0+\beta_1x_{t}+\rho\left(Y_{t-1}-\beta_0-\beta_1x_{t-1}\right)+\epsilon_t\\
    &=\beta_0-\rho\beta_0+\rho Y_{t-1}+\beta_1x_{t}-\beta_1x_{t-1}+\epsilon_t
  \end{aligned}
\]
</div>

ou $Y_t-\rho Y_{t-1}=(1-\rho)\beta_0+\beta_1\left(x_{t}-\rho x_{t-1}\right)+\epsilon_t$, 
de outra forma, $Y_t^*=\beta_0^*+\beta_1x^*_{t}+\epsilon_t$.


> **Sugestão**: Transforme as variáveis e use MQO no novo modelo.

--

**Problemas**: 

1. perdemos a primeira observação;
1. $\rho$ é desconhecido.


---
class: animated, slideInRight

### Exemplo - II

#### **Problema 1.** Pérdida da primeira observação.

Para resolver o primer problema, sugere-se modelar a primeira observação e 
incorporá-la ao modelo.

$$\sqrt{1-\rho^2}\,Y_1=\sqrt{1-\rho^2}\,\beta_0+\sqrt{1-\rho^2}\,\beta_1x_{1}+\sqrt{1-\rho^2}\,\varepsilon_1,$$
ou

$$Y_1^*=\beta_0x_{11}^*+\beta_1x_{12}^*+\varepsilon_1^*,$$
onde $Y_1^*=\sqrt{1-\rho^2}\,Y_1$, $x_{11}^*=\sqrt{1-\rho^2}$, $x_{12}^*=\sqrt{1-\rho^2}x_1$ e 
$\varepsilon_1^*=\sqrt{1-\rho^2}\,\varepsilon_1$.


> Observe que $\mathbb{E}[\varepsilon_1^*]=0$ e 
$\textrm{var}[\varepsilon_1^*]=\textrm{var}\left[\sqrt{1-\rho^2}\,\varepsilon_1\right]=(1-\rho^2)\textrm{var}[\varepsilon_1]=(1-\rho^2)\dfrac{\sigma_\epsilon^2}{1-\rho^2}=\sigma_\epsilon^2$.

---
class: animated, slideInRight

### Exemplo - II

Dessa forma, o novo modelo é dado por $Y^*=\mathbf{X}^*\beta+\varepsilon^*$, onde

<div class="math">
\[Y^*=\left[
\begin{array}{c}
\sqrt{1-\rho^2}\,Y_1 \\
Y_2-\rho Y_1 \\
\vdots\\
Y_n-\rho Y_{n-1}
\end{array}
\right],\quad 
\mathbf{X}^*=\left[
\begin{array}{cc}
\sqrt{1-\rho^2}&\sqrt{1-\rho^2}x_{1} \\
1-\rho&x_{2}-\rho x_{1} \\
\vdots&\vdots\\
1-\rho&x_{n-2}-\rho x_{n-1}
\end{array}
\right] \quad \text{ e } \quad

\varepsilon^*=
\left[
\begin{array}{c}
\sqrt{1-\rho^2}\varepsilon_1 \\
\varepsilon_2 \\
\vdots\\
\varepsilon_n
\end{array}
\right].
\]
</div>

Aplicando MQG, temos que 

$$\widehat{\beta}=\left({\mathbf{X}^*}'\mathbf{X}^*\right)^{-1}{\mathbf{X}^*}'Y^*.$$
Pelo Teorema de Gauss-Markov-Aitken, $\widehat{\beta}$ é o melhor estimador linear 
não-viesado e de mínima variância para $\beta$.


---
class: animated, slideInRight

### Exemplo - III

#### **Problema 2.** $\,\rho$ desconhecido.

Para estimar $\rho$, sugere-se o seguinte procedimento:

1. Tome a equação de autocorrelação $\varepsilon_t=\rho\,\varepsilon_{t-1}+\epsilon_t$;
1. $\varepsilon_t$ pode ser aproximado por $e_t=Y_t-\widehat{\beta}_0-\widehat{\beta_1}x_1$. Dessa forma, $e_t=\rho\,e_{t-1}+u_t$;
1. Estime $\rho$ por MQO,
  
  $$\widehat{\rho}=\dfrac{\sum_{t=2}^ne_te_{t-1}}{\sum_{t=2}^ne_{t-1}^2}.$$
Use $\widehat{\rho}$ para transformar as variáveis e obter $Y^*$ e $\mathbf{X}^*$. Depois calcule
o estimador de MQO para $\beta$, i.e.

$$\widehat{\widehat{\beta}}=\left({\mathbf{X}^*}'\mathbf{X}^*\right)^{-1}{\mathbf{X}^*}'Y^*.$$
> Quando $n$ é suficientemente grande, as propriedades estatísticas de $\widehat{\widehat{\beta}}$ e $\widehat{\beta}$ são bem parecidas. Para pequenas amostras os comportamentos são bem distintos.

---
class: animated, slideInRight

#### Alternativas para estimar $\rho$

* Theil (1971)<sup>1</sup> sugero o estimador dado por $\widehat{\rho}=\dfrac{n-p}{n-1}\dfrac{\sum_{t=2}^ne_te_{t-1}}{\sum_{t=2}^ne_{t-1}^2}.$

* Durbin and Watson (1950, 1951)<sup>2</sup> sugerem o seguinte estimador $\widehat{\rho}=1-\dfrac{d}{2}$, onde

$$d=\dfrac{\sum_{t=2}^n\left(e_t-e_{t-1}\right)^2}{\sum_{t=2}^ne_{t-1}^2}.$$
* Theil & Nagar (1961) sugerem o estimador dado por $\widehat{\rho}=\dfrac{n^2\left(1-\frac{d}{2}\right)+p^2}{n^2-p^2}$.

***
  <font size="3"><sup>1</sup>Theil, H. (1971) Principles of Econometrics. New York: John Wiley & Sons. </font>

  <font size="3"><sup>2</sup>Durbin, J., and Watson, G. S. (1950) Testing for serial correlation in least squares regression (I), Biometrika 37: 409–428.</font> 

  <font size="3"><sup>2</sup>Durbin, J., and Watson, G. S. (1951) Testing for serial correlation in least squares regression (II), Biometrika 38: 159–178.</font>

  <font size="3"><sup>3</sup>Theil, H., & Nagar, A.L. (1961) Testing the Independence of Regression Disturbances. Journal of the American Statistical Association, 56, 793-806.</font>


---
class: animated, slideInRight

### Mais uma alternativa

#### Estime $\rho$ e $\beta$ de forma simultânea

Considere $Y_t-\rho Y_{t-1}=(1-\rho)\beta_0+\beta_1\left(x_{t}-\rho x_{t-1}\right)+\epsilon_t$, dessa forma estime $\rho, \beta_0$ e $\beta_1$ minimizando a soma de quadrados dos erros, i.e.,

$$S(\rho,\beta_0,\beta_1)=\sum_{t=2}^n \left(Y_t-\rho Y_{t-1}-(1-\rho)\beta_0-\beta_1\left(x_{t}-\rho x_{t-1}\right)\right)^2.$$
Para incorporar a primeira observação, minimize

$$S(\rho,\beta_0,\beta_1)=S^*(\rho,\beta_0,\beta_1)+\left(\sqrt{1-\rho^2}\,\varepsilon_1\right)^2=S^*(\rho,\beta_0,\beta_1)+(1-\rho^2)(Y_1-\beta_0-\beta_1x_1).$$
Daí, os estimadores não tem forma fechada e precisam ser obtidos usando algoritmos de otimização
não-linear, por exemplo: Newton-Rapshon, BHHH, BFGS, DFP, etc.

---
class: inverse, hide-logo, middle, center

# Métodos para detecção

---
class: animated, slideInRight

### Autocorrelação

#### Métodos para detecção - I

* **Estatística de Durbin-Watson**: Proposto por Durbin & Watson (1950)<sup>1</sup>, a estatística
permitirá avaliar a ausência de autocorrelação com as seguintes hipóteses
$H_0: \rho=0$ vs $H_1:\rho\ne0$. A estatística $d$ é dada por

$$d=\dfrac{\sum_{t=2}^n\left(e_t-e_{t-1}\right)^2}{\sum_{t=2}^ne_{t-1}^2}.$$
> A estatística $d$ pode ser aproximada pela expressão $d\approx 2(1-\widehat{\rho})$. Dessa forma,
as realizações de $d$ variam no intervalo $[0,4]$.
  - Quando a realização de $d$ é próxima de $2$ existe evidência de ausência de autocorrelação;
  - Quando a realização de $d$ é próxima de $0$ existe evidência de autocorrelação positiva;
  - Quando a realização de $d$ é próxima de $4$ existe evidência de autocorrelação negativa;

***

<font size="3"><sup>1</sup>Durbin, J., and Watson, G. S. (1950) Testing for serial correlation in least squares regression (I), Biometrika 37: 409–428.</font> 

---
class: animated, slideInRight

### Autocorrelação

#### Métodos para detecção - II

* **Teste de Durbin-Watson**: Para a construção do teste requer-se a definição
de uma região crítica, obtida a partir da distribuição da estatística $d$.

  Durbin & Watson (1950) mostraram que, sob $H_0$, a distribuição da estatística 
$d$ depende da matriz $\mathbf{X}$. Isso é um problema porque impede a tabulação 
dos valores críticos. Para resolver o impasse, os autores encontraram duas 
estatísticas $d_L$ e $d_U$ cujas distribuições sob $H_0$ não dependem de $\mathbf{X}$
e tais que $d_L<d<d_U$ com probabilidade $1$.

  **Cenário 1**: Considere $H_0:\rho=0\,$ vs $\, H_1:\rho>0$. 
  
  - Rejeita-se $H_0$ se  $d<d_{L_c}$, onde $d_{L_c}$ é o quantil da distribuição 
  da estatística $d_L$ (sob $H_0$) fixando um nivel de significância $\alpha$;
  - No caso em que $d>d_{U_c}$, não há evidencia suficiente para rejeitar $H_0$;
  - Se $d_{L_c}<d<d_{U_c}$ o teste é inconclusivo.

>  Uma vantagem ao considerar $d_{L_c}$ e $d_{U_c}$ é que dependem apenas de $n$ e
$p$. Dessa forma, podemos tabelar os valores críticos.

---
class: animated, slideInRight

### Autocorrelação

#### Métodos para detecção - III

**Cenário 2**: Considere $H_0:\rho=0\,$ vs $\, H_1:\rho<0$.

- Rejeita-se $H_0$ se  $d>4-d_{L_c}$;
  - No caso em que $d<4-d_{U_c}$, não há evidencia suficiente para rejeitar $H_0$;
  - Se $4-d_{U_c}<d<4-d_{L_c}$ o teste é inconclusivo.

**Algumas considerações**

O teste de Durbin-Watson ao longo dos anos tornou-se tão respeitado que os 
usuários muitas vezes se esquecem das hipóteses que o fundamentam. Em especial, 
das seguintes hipóteses:
  1. As covariáveis são determinísticas; 
  1. O termo de erro segue uma distribuição normal; 
  1. Os modelos de regressão não incluem os valores defasados das covariáveis; 
  1. Apenas a autocorrelação de primeira ordem é levada em consideração;
  1. A estatística $d$ pode não indicar necessariamente autocorrelação. Em vez 
  disso, ela pode ser indicação de omissão de variáveis relevantes no modelo.
  
---
class: animated, slideInRight

### Autocorrelação

#### Métodos para detecção - IV

* **Teste de Breusch e Godfrey (BG)**: Proposto por Breusch (1978)<sup>1</sup> e 
Godfrey (1978)<sup>2</sup>, o teste é baseado em multiplicadores de Lagrange e
o objetivo é avaliar as seguintes hipóteses:

  $H_0:$ "Não autocorrelação" vs $\, H_1:$ "O termo de erro segue um modelo ARMA(p,q)".  

  Para ilustrar o mecanismo por trás do teste BG, considere o seguinte modelo
  
  $$Y_t=\beta_0+\beta_1x_t+\varepsilon_t, \quad t=1,2,3,\ldots, n,$$
  onde $\varepsilon_t=\rho_1\varepsilon_{t-1}+\rho_2\varepsilon_{t-2}+\cdots+\rho_p\varepsilon_{t-p}+\epsilon_t$, i.e., o termo de erro segue um modelo autorregressivo de ordem $p$ e 
  $\{\epsilon_t\}$ um processo de ruído branco.


***

<font size="3"><sup>1</sup>BREUSCH, T. S. (1978) Testing for autocorrelation in dynamic linear models. Australian Economic Papers, 17, p. 334–355.</font> 

<font size="3"><sup>2</sup>GODFREY, L. G. (1978) Testing against general autoregressive and moving average error models when the regressor includes lagged dependent variables. Econometrica, v. 46, p. 1293–1302.</font> 


---
class: animated, slideInRight

### Autocorrelação

#### Métodos para detecção - IV

Dessa forma, a hipótese nula é $H_0: \rho_1=\rho_2=\cdots=\rho_p=0$. Daí,

1. Obtenha os resíduos usando o estimador de MQO e denote eles como 
$\widehat{\varepsilon}_t$;
1. Ajuste o modelo de regressão auxiliar de $\widehat{\varepsilon}_t$ contra $x_t$ e 
$\widehat{\varepsilon}_{t-1},\widehat{\varepsilon}_{t-2},\ldots,\widehat{\varepsilon}_{t-p}$, i.e.

  $$\widehat{\varepsilon}_t=\alpha_1+\alpha_2x_t+\widehat{\rho}_1\widehat{\varepsilon}_{t-1}+\widehat{\rho}_2\widehat{\varepsilon}_{t-2}+\cdots+\widehat{\rho}_p\widehat{\varepsilon}_{t-p}+u_t.$$
  e calcule o coeficiente de determinação;
  > Caso que houver mais covariáveis no modelo original, as mesmas devem ser incluídas
  na regressão auxiliar.

3. A estatística $(n-p)R^2$ é assintotocamente $\chi^2_p$. Dessa forma, rejeita-se 
$H_0$ se $(n-p)R^2>\chi^2_{p,1-\alpha}$.


---
class: animated, slideInRight

### Autocorrelação

#### Métodos para detecção - IV

* **Teste Box-Pierce**: Proposto por Box & Pierce (1970)<sup>1</sup>, o teste
avalia hipótese de não-autocorrelação com a seguinte estatística de teste

  $$Q_{BP}=n\sum_{k=1}^h\widehat{\rho}^2_k,$$
  onde $\widehat{\rho}_k=\dfrac{\sum_{t=k+1}^ne_te_{t-k}}{\sum_{t=1}^ne^2_t}$ e 
  $h$ o número de defasagens que estão sendo consideradas na avaliação.
  
  Rejeita-se $H_0$, se $Q_{BP}>\chi^2_{h, 1-\alpha}$.


***

<font size="3"><sup>1</sup>Box, G. E. P. and Pierce, D. A. (1970) Distribution of Residual Autocorrelations in Autoregressive-Integrated Moving Average Time Series Models. Journal of the American Statistical Association, 65: 1509–1526.</font> 


---
class: animated, slideInRight

class: animated, slideInRight

### Autocorrelação

#### Métodos para detecção - IV

* **Teste Ljung-Box**: Proposto por Ljung & Box (1978)<sup>1</sup>, os autores
propõem uma variante do teste de Box-Pierce usando a seguinte estatística de 
teste

  $$Q=n(n+2)\sum_{k=1}^h\dfrac{\widehat{\rho}^2_k}{n-k}.$$
  Rejeita-se $H_0$, se $Q_{LB}>\chi^2_{h, 1-\alpha}$.


> O teste de Ljung-Box apresenta melhor desempenho mesmo para tamanho de amostra
pequena.


***

<font size="3"><sup>1</sup>Ljung, G. M. and Box, G. E. P. (1978). On a Measure of a Lack of Fit in Time Series Models. Biometrika 65 (2): 297-303.</font> 

---
class: animated, lightSpeedIn
# Referências

```{r refs, echo=FALSE, results="asis"}
PrintBibliography(myBib, .opts = list(check.entries = FALSE))
```

---
class: animated, hide-logo, bounceInDown
## Política de proteção aos direitos autorais

> <span style="color:grey">O conteúdo disponível consiste em material protegido pela legislação brasileira, sendo certo que, por ser
o detentor dos direitos sobre o conteúdo disponível na plataforma, o **LECON** e o **NEAEST** detém direito
exclusivo de usar, fruir e dispor de sua obra, conforme Artigo 5<sup>o</sup>, inciso XXVII, da Constituição Federal
e os Artigos 7<sup>o</sup> e 28<sup>o</sup>, da Lei 9.610/98.
A divulgação e/ou veiculação do conteúdo em sites diferentes à plataforma e sem a devida autorização do
**LECON** e o **NEAEST**, pode configurar violação de direito autoral, nos termos da Lei 9.610/98, inclusive podendo
caracterizar conduta criminosa, conforme Artigo 184<sup>o</sup>, §1<sup>o</sup> a 3<sup>o</sup>, do Código Penal.
É considerada como contrafação a reprodução não autorizada, integral ou parcial, de todo e qualquer
conteúdo disponível na plataforma.</span>

.pull-left[
```{r, out.width='50%', fig.align='center', fig.cap='',echo=FALSE}
knitr::include_graphics("images/logo_lecon.png")
```
]
.pull-right[
```{r, out.width='50%', fig.align='center', fig.cap='',echo=FALSE}
knitr::include_graphics("images/logo_neaest.png")
```
]
<br></br>
.center[
[https://lecon.ufes.br](https://lecon.ufes.br/) &emsp; &emsp;  &emsp; &emsp; [https://analytics.ufes.br](https://analytics.ufes.br)
]


<font size="2"><span style="color:grey">Material elaborado pela equipe LECON/NEAEST: 
Alessandro J. Q. Sarnaglia, Bartolomeu Zamprogno, Fabio A. Fajardo, Luciana G. de Godoi 
e Nátaly A. Jiménez.</span></font>
