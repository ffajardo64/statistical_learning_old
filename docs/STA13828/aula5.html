<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Análise de Séries Temporais</title>
    <meta charset="utf-8" />
    <meta name="author" content="  " />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.7.0/animate.min.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Análise de Séries Temporais
]
.subtitle[
## Modelagem em processos com representação ARMA(p,q) - I
]
.author[
### <br/><br/>
]
.institute[
### LECON/DEST - UFES
]
.date[
### Vitória. ES - 09/12/2022
]

---

[//]: &lt;&gt; (https://pkg.garrickadenbuie.com/countdown/#1)
[//]: &lt;&gt; (https://github.com/animate-css/animate.css/blob/main/animate.css)
[//]: &lt;&gt; (xaringan::inf_mr())

class: animated, fadeIn







&lt;style&gt; body {text-align: justify} &lt;/style&gt; &lt;!-- Justify text. --&gt;


### Modelagem em processos com representação ARMA(p,q)

Na construção de modelos para séries temporais, 
Box and Jenkins (1976) propuseram uma metodologia baseada
em quatro etapas:

* **Identificação**: Refere-se à determinação dos valores das ordens da
representação em diferenças, i.e. se a representação adequada é ARIMA,
então a ideia é determinar as ordens `\(p, d\)` e `\(q\)`;

* **Estimação**: Refere-se à estimação dos parâmetros envolvidos na 
representação. Existem vários métodos para estimação dos parâmetros;

* **Verificação**: Nesta etapa deve-se avaliar se o ajuste é adequado,
i.e., se todas as suposições são satisfeitas. Caso as suposições não 
sejam satisfeitas as etapas anteriores devem ser repetidas;

* **Uso do modelo**: Cumpridas as etapas anteriores, procede-se no
uso do modelo ajustado. O modelo ajustado pode ser utilizado para
predição, controle de processos, simulação ou simplesmente para estudar
a dinâmica dos fenômenos sob estudo.

&lt;br /&gt;

***
Para detalhes, sugere-se a leitura do livro de Brockwell and Davis (2016),
Wei (2005) e Hamilton (1994).

---
class: animated, fadeUp

### Modelagem em processos com representação ARMA(p,q)

&lt;img src="images/etapas_BJ.png" width="60%" style="display: block; margin: auto;" /&gt;

---
class: inverse, hide-logo, middle, center

## 1. Identificação


---
class: animated, fadeInDown

### Transformações

Caso a não-estacionariedade seja causada pela variância do processo, i.e.,
a variância do processo não é constante, então uma primeira tentativa
é usar uma transformação de potência. De acordo com Bartlett (1974)&lt;sup&gt;1&lt;/sup&gt;, 
esse tipo de transformação permite "*estabilizar*" a variância do 
processo, mesmo antes de remover a tendência polinomial do mesmo.

Suponha que a variância do processo `\(\{X_t\}\)` pode ser escrita em função 
da sua média, i.e.,
`$$\sigma^2_t=f(\mu_t),$$`
dessa forma, se `\(T(\cdot)\)` é uma função, cuja primeira derivada existe,
então, a função `\(T\)` pode ser aproximada por
`$$T(X_t)=T(\mu_t)+\left(\frac{dT}{dX_t}\Biggr|_{X_t=\mu_t}\right)(X_t-\mu_t).$$`

&lt;br /&gt;&lt;br /&gt;

***
&lt;font size=3&gt;Barttlet, M. S. (1974). The use of transformations. Biometrika, 3, 39-52.&lt;/font&gt;

---
class: animated, fadeIn

### Transformações

Daí, uma aproximação linear para a variância do processo 
`\(\{T(X_t)\}\)` é dada por
`$$\mathrm{var}(T)=\left(\frac{dT}{dX_t}\Biggr|_{X_t=\mu_t}\right)^2f(\mu_t).$$`

Já que o objetivo é tornar a variância constante, então

`$$\frac{dT}{dX_t}\Biggr|_{X_t=\mu_t}=\sqrt{\frac{\mathrm{var}(T)}{f(\mu_t)}}=\frac{C}{\sqrt{f(\mu_t)}}$$`
e, dessa forma

`$$T(\mu_t)=\int\frac{C}{\sqrt{f(\mu_t)}}\,d\mu_t.$$`
De acordo com o resultado, faz-se necessário conhecer a forma analítica 
da função `\(f\)`, para determinar o tipo de transformação que estabilize 
a variância do processo.


---
class: animated, fadeIn

### Transformações

Se a variável `\(X_t\)` é positiva, podemos supor que a variância do
processo é proporcional a `\(\mu_t^{2(1-\lambda)}\)`, onde `\(\lambda\)` é um
valor constante, em outras palavras, `\(f(\mu_t)\propto \mu_t^{2(1-\lambda)}\)`.
Daí, 

&lt;div class="math"&gt;
  \begin{align*}
  T(\mu_t)&amp;\propto\begin{cases}
  \mu_t^\lambda, &amp; \text{ se } \lambda\ne 0;\\
  \log(\mu_t), &amp; \text{ se } \lambda=0.
  \end{cases}
  \end{align*}
&lt;/div&gt;

Dessa forma, o último resultado sugere que, a função `\(T(\cdot)\)` 
que torna a variância do processo aproximadamente constante é dada por

&lt;div class="math"&gt;
  \begin{align*}
  T(X_t)&amp;=\begin{cases}
  X_t^\lambda, &amp; \text{ se } \lambda\ne 0;\\
  \log(X_t), &amp; \text{ se } \lambda=0.
  \end{cases}
  \end{align*}
&lt;/div&gt;

Na prática, para `\(t=1,2,\ldots,n\)`, deve-se escolher um valor `\(\lambda\)`, 
tal que
`$$\frac{\sigma_t}{\mu_t^{1-\lambda}}\equiv \text{ constante}.$$`

---
class: animated, fadeIn
### Transformações

Para estimar `\(\sigma_t\)`, Guerrero (1993)&lt;sup&gt;1&lt;/sup&gt; sugere o seguinte
método:

1. A partir da `\(n\)` observações, crie `\(H\)` grupos homogêneos de 
tamanho `\(R=\frac{n-N}{H}\)`, onde `\(N\)` é o número de observações 
desconsideradas para os cálculos, tal que `\(0\le N&lt; R\)`. As `\(N\)` 
observações podem ser do início ou do final da série temporal;

1. Calcule a média e o desvio padrão dentro de cada grupo. Dessa forma,
teremos `\(H\)` pares `\((S_h,\bar{X}_h)\)`, com `\(h=1,2,\ldots, H\)`, tal que,
se `\(X_{h,r}\)` é a `\(r\)`-ésima observação do grupo `\(H\)`, então
`$$\bar{X}_h=\frac{1}{R}\sum_{r=1}^RX_{h,r}$$`
e 
`$$S_h=\sqrt{\frac{1}{R-1}\sum_{r=1}^R\left(X_{h,r}-\bar{X}_h\right)^2}.$$`

***
&lt;font size=3&gt;Guerrero, V. (1993). Time series analysis supported by power transformations. Journal of forecasting, 12, 37-48.&lt;/font&gt;


---
class: animated, fadeIn

### Transformações

| &lt;td colspan=5&gt; `\(\hspace{5cm} \lambda\)` &lt;/td&gt;
|:-------:|:----:|:------:|:-----:|:-----:|:-----:|
|   Grupo |   -1 |  -0.5  |   0   |  0.5  |   1   |
|   1     | `\(\frac{S_1}{\bar{X}_1^2}\)` | `\(\frac{S_1}{\bar{X}_1^{1.5}}\)` | `\(\frac{S_1}{\bar{X}_1}\)` | `\(\frac{S_1}{\bar{X}_1^{0.5}}\)` | `\(S_1\)` |
|   2     | `\(\frac{S_2}{\bar{X}_2^2}\)` | `\(\frac{S_2}{\bar{X}_2^{1.5}}\)` | `\(\frac{S_2}{\bar{X}_2}\)` | `\(\frac{S_2}{\bar{X}_2^{0.5}}\)` | `\(S_2\)` |
| `\(\vdots\)`| `\(\vdots\)` | | | `\(\vdots\)`| `\(\vdots\)`
|   `\(h\)`   | `\(\frac{S_h}{\bar{X}_h^2}\)` | `\(\frac{S_h}{\bar{X}_h^{1.5}}\)` | `\(\frac{S_h}{\bar{X}_h}\)` | `\(\frac{S_h}{\bar{X}_h^{0.5}}\)` | `\(S_h\)` |
| `\(\vdots\)`| `\(\vdots\)` | | | `\(\vdots\)`| `\(\vdots\)`
|    `\(H\)`  | `\(\frac{S_H}{\bar{X}_H^2}\)` | `\(\frac{S_H}{\bar{X}_H^{1.5}}\)` | `\(\frac{S_H}{\bar{X}_H}\)` | `\(\frac{S_H}{\bar{X}_H^{0.5}}\)` | `\(S_H\)` |
| Coef. de variação| `\(\mathrm{CV}(-1)\)` | `\(\mathrm{CV}(-0.5)\)` |  `\(\mathrm{CV}(0)\)` | `\(\mathrm{CV}(0.5)\)` | `\(\mathrm{CV}(1)\)` |


---
class: animated, fadeLeft

### Transformações

O coeficiente de variação calcula-se como 
`$$\mathrm{CV}(\lambda)=\frac{dp(\lambda)}{M(\lambda)}=\frac{\sqrt{\frac{1}{H-1}\sum_{h=1}^H\left[\frac{S_h}{\bar{X}_h^{1-\lambda}}-\frac{1}{H}\sum_{h=1}^H\left(\frac{S_h}{\bar{X}_h^{1-\lambda}}\right)\right]^2}}{\frac{1}{H}\sum_{h=1}^H\left(\frac{S_h}{\bar{X}_h^{1-\lambda}}\right)}.$$`
Daí, o valor de `\(\lambda\)` selecionado é aquel que proporcione o menor
valor do coeficiente de variação. Assim, o processo resultante é 
dado por

&lt;div class="math"&gt;
  \begin{align*}
  T(X_t)&amp;=\begin{cases}
  X_t^\lambda, &amp; \text{ se } \lambda\ne 0;\\
  \log(X_t), &amp; \text{ se } \lambda=0.
  \end{cases}
  \end{align*}
&lt;/div&gt;


---
class: inverse, hide-logo, middle, center

## Testes de raízes unitárias


---
class: animated, fadeLeft

### Testes de raíz unitária - I

* O principal objetivo é determinar as ordens dos polinômios 
autorregressivos e de médias móveis, assim como o número de diferênças
necessárias para atingir a estacionariedade do processo. Em geral, a
pretensão é identificar e remover as possíveis tendências que tornam o
processo não-estacionário e assim identificar as ordens `\(p\)` e `\(q\)`;

* Os **Testes de raízes unitárias** são uma forma mais formal de 
verificar o grau de integração do processo sob estudo. Na análise de
séries temporais algumas técnicas permitem criar uma suspeita a respeito
da não-estacionariedade do processo gerador dos dados, por exemplo:
A inspeção do gráfico da série vs tempo ou o gráfico da função de 
autocorrelação amostral;

* No caso em que `\(d\ge 0\)`, dizemos que a o processo é não-estacionário
ou *integrado* de ordem `\(d\)`, denotado por `\(I(d)\)`;

* A identificação do parâmetro `\(d\)` é de grande relevância, pois a
sobrediferenciação do processo trará dificuldade na identificação das
ordens corretas dos polinômios autorregressivo e de médias móveis,
comprometimento das previsões e aumento nos erros de previsão. 
A sobrediferenciação pode acarretar a presença de raízes unitárias no
polinômio de médias móveis;

* Por outra parte, a subdiferenciação traz 
consequências mais graves, pois considerar
um processo estacionário, sendo ele não-estacionário, invalida por completo 
a metodologia aplicada.


---
class: animated, fadeRight

### Testes de raíz unitária - I

* **Definição**. Um processo `\(\{X_t\}\)` diz-se com representação
ARIMA `\(\!(p,d,q)\)` se `\(Y_t=(1-B)^dX_t\)`, `\(d\)` inteiro não-negativo, é um 
processo ARMA `\((p,q)\)` causal.

&gt; * De acordo com a definição, um processo com representação ARIMA, 
satisfaz a seguinte equação em diferenças
`$$\phi(B)(1-B)^dX_t=\theta(B)Z_t,$$`
onde `\(\{Z_t\}\)` é um processo de ruído branco com média zero e 
variância `\(\sigma^2\)`, `\(\phi(B)\)` e `\(\theta(B)\)` representam os polinômios 
autorregressivos e de médias móveis, respectivamente. `\(\phi(z)\ne0\)`
para `\(|z|\le1\)`;

&gt; * O processo `\(\{X_t\}\)` é estacionário *see* `\(d=0\)`, nesse caso o processo
terá representação ARMA `\(\!(p,q)\)`;

&gt; * Processos com representação ARIMA são adequados para estudar séries
temporais com  tendência


---
class: animated, fadeDown

### Testes de raíz unitária - I

O problema de raízes unitária em processos com representação ARMA aparece
quando o polinômio autorregressivo apresenta uma raíz sobre círculo 
unitário.

Considere um processo `\(\{X_t\}\)` com representação autorregressiva de 
primeira ordem, i.e. `$$X_t-\mu=\phi_1(X_{t-1}-\mu)+Z_t,$$`
onde `\(\{Z_t\}\)` é um processo de ruído branco gaussiano com média zero e 
variância `\(\sigma^2\)`, com `\(\mu=\mathbb{E}X_t\)` e `\(|\phi_1|&lt;1\)`.
  
Seja `\(\hat{\phi}_1^{MV}\)` o estimador de máxima verossimilhança de 
`\(\phi_1\)`, então `\(\hat{\phi}_1^{MV}\)` é assintoticamente normal com
média `\(\phi\)` e variância `\(\frac{1-\phi_1^2}{n}\)`.

Para testar `\(H_0:\phi_1=\phi\)` contra `\(H_1:\phi_1\ne\phi\)`, podemos usar
a estatística de teste dada por
`$$\frac{\hat{\phi}_1^{MV}-\phi}{ep\left(\hat{\phi}_1^{MV}\right)},$$`
sob `\(H_0\)` segue uma distribuição `\(t\)`-student.

---
class: animated, fadeIn

### Testes de raíz unitária - I

* **Teste de Dickey-Fuller (DF)**: Proposto por Dickey e Fuller (1979)&lt;sup&gt;1&lt;/sup&gt;
tem como objetivo avaliar a presença de tendência no processo sob estudo.

  **Idea do teste**
  
  **Caso 1 (sem termo constante).** Considere um processo `\(\{X_t\}\)` com 
  representação autorregressiva de primeira ordem, i.e.
  `$$X_t=\phi_1X_{t-1}+Z_t,$$`
  onde `\(\{Z_t\}\)` é um processo de ruído branco gaussiano com média zero e 
  variância `\(\sigma^2\)`. Dessa forma, o processo `\(\{X_t\}\)` tem raíz
  unitária se `\(\phi_1=1\)`, pois 
  `$$\nabla X_t=X_t-X_{t-1}=\phi_1^*X_{t-1}+Z_t,$$`
  onde `\(\phi_1^*=\phi_1-1\)`.
  
  
 &lt;br /&gt;
 
***

&lt;font size=3&gt;Dickey, D. A., &amp; Fuller, W. A. (1979). Distribution of the estimators for autoregressive time series with a unit root. Journal of American Statistical Association, 74,
427–431.&lt;/font&gt;


---
class: animated, fadeUp

### Testes de raíz unitária - I

O parâmetro `\(\phi_1^*\)` pode ser estimado por mínimos quadrados 
ordinários, sendo o erro padrão dado por
`$$ep\left(\phi_1^*\right)=\frac{S}{\left(\sum_{t=2}^nX_{t-1}^2\right)^{1/2}},$$`
onde `\(S^2=\frac{1}{n-2}\sum_{t=2}^n\left(\nabla X_t-\hat{\phi^*}_1X_{t-1}\right)^2\)` é
o estimador de `\(\sigma^2\)` na regressão.

Dessa forma, considerando a hipótese nula `\(H_0: \phi_1^*=0\)` contra 
`\(H_1: \phi_1^*&lt;0\)`, Dickey e Fuller (1979) sugerem como estatística de teste 
tipo `\(t\)` dada por
`$$\hat{\tau}=\frac{\hat{\phi^*}_1}{ep\left(\phi_1^*\right)}=\frac{\hat{\phi}_1-1}{\left(\frac{S^2}{\sum_{t=2}^nX_{t-1}^2}\right)^{1/2}}.$$`
A distribuição assintótica da estatística `\(\tau\)` é apresentada 
pelos autores e os quantis da distribuição limite para `\(\alpha=0.01, 0.05\)`
e `\(0.10\)` são dados por `\(-2.58, -1.95\)` e `\(-1.62\)`, respectivamente.


---
class: animated, fadeDown

### Testes de raíz unitária - I

Supondo que `\(\{Z_t\}\)` é um processo de ruído branco gaussiano com
média `\(0\)` e variância `\(\sigma^2\)`, então
`$$n\left(\hat{\phi_1^*}-1\right)\overset{D}{\longrightarrow}\frac{\frac12\left([W(1)]^2-1\right)}{\int_0^1[W(r)]^2\,dr},$$`
onde `\(W(r)\)` é o movimento Browniano padrão, i.e., `\(W(t)\)` segue uma 
distribuição normal com média zero e variância `\(t\)`, para todo `\(t\)`. Daí
`$$\tau\overset{D}{\longrightarrow}\frac{\frac12\left([W(1)]^2-1\right)}{\left(\int_0^1[W(r)]^2\,dr\right)^{1/2}}.$$`

**Caso 2 (com termo constante).** Considere um processo `\(\{X_t\}\)` com 
  representação autorregressiva de primeira ordem, i.e.
  `$$X_t-\mu=\phi_1\left(X_{t-1}-\mu\right)+Z_t,$$`
  onde `\(\{Z_t\}\)` é um processo de ruído branco gaussiano com média zero e 
  variância `\(\sigma^2\)`.

---
class: animated, fadeLeft

### Testes de raíz unitária - I

Dessa forma, o processo `\(\{X_t\}\)` tem raíz unitária se `\(\phi_1=1\)`, pois 
  `$$\nabla X_t=X_t-X_{t-1}=\phi_0^*+\phi_1^*X_{t-1}+Z_t,$$`
onde `\(\phi_0^*=\mu(1-\phi_1)\)`, `\(\phi_1^*=\phi_1-1\)` e `\(\mathbb{E}X_t=\mu\)`.

Seja `\(\hat{\phi^*}_1\)` o estimador de mínimos quadrados de `\(\phi_1^*\)`,
com erro padrão dado por
`$$ep\left(\hat{\phi}_1^*\right)=\frac{S}{\left(\sum_{t=2}^n\left(X_{t-1}-\bar{X}\right)^2\right)^{1/2}},$$`
onde `\(S^2=\frac{1}{n-3}\sum_{t=2}^n\left(\nabla X_t-\hat{\phi_0^*}-\hat{\phi_1^*}X_{t-1}\right)^2\)` e `\(\bar{X}\)` a média
amostral de `\(X_1,X_2,\ldots, X_{n-1}\)`.

---
class: animated, fadeLeft

### Testes de raíz unitária - I

Dessa forma, considerando a hipótese nula `\(H_0: \phi_1^*=0\)` contra 
`\(H_1: \phi_1^*&lt;0\)`, Dickey e Fuller (1979) sugerem como estatística de teste 
tipo `\(t\)` dada por
`$$\hat{\tau_\mu}=\frac{\hat{\phi_1^*}}{ep\left(\phi_1^*\right)}=\frac{\hat{\phi}_1-1}{\left(\frac{S^2}{\sum_{t=2}^n\left(X_{t-1}-\bar{X}\right)^2}\right)^{1/2}}.$$`
A distribuição assintótica da estatística `\(\tau_\mu\)` é apresentada 
pelos autores e os quantis da distribuição limite para `\(\alpha=0.01, 0.05\)`
e `\(0.10\)` são dados por `\(-3.43, -2.86\)` e `\(-2.57\)`, respectivamente.

Supondo que `\(\{Z_t\}\)` é um processo de ruído branco gaussiano com
média `\(0\)` e variância `\(\sigma^2\)`, então
`$$n\left(\hat{\phi_1^*}-1\right)\overset{D}{\longrightarrow}\frac{\frac12\left([W(1)]^2-1\right)-W(1)\int_0^1W(r)\,dr}{\int_0^1[W(r)]^2\,dr-\left(\int_0^1W(r)\,dr\right)^2},$$`
onde `\(W(r)\)` é o movimento Browniano padrão, i.e., `\(W(t)\)` segue uma 
distribuição normal com média zero e variância `\(t\)`, para todo `\(t\)`. 


---
class: animated, fadeInDown

### Testes de raíz unitária - I
Daí
`$$\tau_\mu\overset{D}{\longrightarrow}\frac{\frac12\left([W(1)]^2-1\right)-W(1)\int_0^1W(r)\,dr}{\left(\int_0^1[W(r)]^2\,dr-\left(\int_0^1W(r)\,dr\right)^2\right)^{1/2}}.$$`

**Caso 3 (com termo constante e com tendência determinística).** 
Considere um processo `\(\{X_t\}\)` com 
  representação autorregressiva de primeira ordem, i.e.
  `$$X_t-\mu=\phi_1\left(X_{t-1}-\mu\right)+\phi_2t+Z_t,$$`
onde `\(\{Z_t\}\)` é um processo de ruído branco gaussiano com média zero e 
variância `\(\sigma^2\)`.

Os detalhes para a distribuição assintótica da estatística de teste são
apresentados na p. 497 do livro de Hamilton (1994).

---
class: animated, fadeLeft

### Testes de raíz unitária - II

*  Na literatura existem outras alternativas para avaliar a 
não-estacionariedade de um processo, baseadas no teste de Dickey e 
Fuller, temos por exemplo:
  - **Teste de Phillips-Perron (PP)**: [Phillips e Perron (1988)](http://cowles.yale.edu/sites/default/files/files/pub/d07/d0795-r.pdf)&lt;sup&gt;1&lt;/sup&gt;
  propõem generalizar os resultados de Dickey e Fuller para um cenário 
  que contempla a ocorrência de mudanças na estrutura da dinâmica do 
  processo. Em particular, se um processo estacionários apresenta 
  uma mudança no seu nível, então os teste de DF indicarão que o processo
  é `\(I(1)\)`. Os autores sugerem uma alteração nas estatísticas de teste 
  para considerar o cenário acima. Contudo, mostram que as distribuições 
  limites são as mesmas que nos cenários estudados para o teste de DF.
  
      &gt; Assintoticamente os testes de DF e PP são equivalentes, mas diferem
  substancialmente em amostras finitas devido às diferentes maneiras 
  como eles corrigem para correlação serial na regressão de teste. 
  
      &gt; Em geral, o poder dos testes de raiz unitária diminui à medida 
      que termos determinísticos são adicionados às regressões do teste. 
      Ou seja, testes que incluem uma constante e tendência na regressão 
      do teste têm menos poder do que testes que apenas incluem uma 
      constante. 


***
&lt;font size=3&gt;&lt;sup&gt;1&lt;/sup&gt;Phillips, P. C. B.; Perron, P. (1988). Testing for a Unit Root in Time Series Regression. Biometrika. 75 (2): 335–346.&lt;/font&gt;



---
class: animated, fadeIn

### Testes de raíz unitária - II

* **Teste KPSS**: Proposto
por Kwiatkowski, Phillips, Schmidt and Shin (1992)&lt;sup&gt;1&lt;/sup&gt;,
o teste é usado para avaliar a hipótese de estacionáriedade do processo 
em torno de uma tendência determinística contra a alternativa de uma 
raiz unitária. A diferênça dos testes de raiz unitária DF e PP, cuja 
hipótese nula é que o processo é `\(I(1)\)`. O teste KPSS avalia a 
estacionariedade do processo, i.e., se o processo é `\(I(0)\)`.
  Os autores propõem o teste usando o seguinte modelo
  
  &lt;div class="math"&gt;
    \begin{align*}
      X_t&amp;=\beta'\mathbf{D}_t+\mu_t+u_t\\
      \mu_t&amp;=\mu_{t-1}+\varepsilon_t,
    \end{align*}
  &lt;/div&gt;
  
  onde `\(\{\varepsilon_t\}\)` é um processo de ruído branco com média `\(0\)` e 
  variância `\(\sigma^2\)`, `\(\mathbf{D}_t\)` representa as componentes 
  deterministicas, `\(\{u_t\}\)` é `\(I(0)\)` e pode ser heteroscedástico, 
  `\(\{\mu_t\}\)` é um passeio aleatório.
  
  O teste KPSS, a ausência de raiz unitária não confirma a 
  estacionariedade do processo, mas, por definição, de estacionariedade 
  em tendência. 
  
  &gt; Para outras alternativas, sugere-se a leitura do material escrito
  por Eric Zivot intitulado **[Unit Root Tests](https://bit.ly/3VB3FEV)**.
  
***
&lt;font size=3&gt;&lt;sup&gt;1&lt;/sup&gt;Kwiatkowski, D.; Phillips, P. C. B.; Schmidt, P.; Shin, Y. (1992). "Testing the null hypothesis of stationarity against the alternative of a unit root". Journal of Econometrics. 54 (1–3): 159–178.&lt;/font&gt;

---
class: inverse, hide-logo, middle, center

## Identificação das ordens `\(p\)` e `\(q\)`

---
class: animated, fadeInDown

### Função de autocovariância amostral
No estudo de processos estacionários de segunda ordem as funções de
autocovariância e/ou autocorrelação desempenham um papel fundamental
na análise inferencial dos processos. 

* **Definição**: Seja `\(\{X_t\}\)` um processo estacionário de segunda 
ordem, o estimador de momentos para a média do processo, `\(\mu\)`, é
a média amostral dada por
`$$\bar{X}_n=\frac{1}{n}\sum_{j=1}^nX_j.$$`

O estimador `\(\bar{X}_n\)` é não-viesado para `\(\mu\)`, i.e., `\(\mathbb{E}\left[\bar{X}_n\right]=\mu\)`
e o Erro Quadrático Médio (EQM) do estimador é dado por
&lt;div class="math"&gt;
  \begin{align*}
    \mathbb{E}\left[\bar{X}_n-\mu\right]^2&amp;=\mathrm{var}\left(\bar{X}\right)=
    \frac{1}{n^2}\sum_{i=1}^n\sum_{j=1}^n\mathrm{cov}\left(X_i,X_j\right)=
    \frac{1}{n^2}\sum_{h=-n}^n(n-|h|)\gamma(h)\\
    &amp;=\frac1n\sum_{h=-n}^n\left(1-\frac{|h|}{n}\right)\gamma(h).
  \end{align*}
&lt;/div&gt;

---
class: animated, fadeIn

### Função de autocovariância amostral

* **Teorema.** Seja `\(\{X_t\}\)` um processo estacionário de segunda ordem
com média `\(\mu\)` e função de autocovariância `\(\gamma(\cdot)\)`, então, quando
`\(n\to\infty\)`
  
  - `\(\mathrm{var}\left(\bar{X}_n\right)=\mathbb{E}\left[\bar{X}_n-\mu\right]^2\longrightarrow0\)` se `\(\gamma(n)\to0\)`;
  - `\(n\mathbb{E}\left[\bar{X}_n-\mu\right]^2\longrightarrow\sum_{|h|&lt;\infty}\gamma(h)\)` 
  se `\(\sum_{h=-\infty}^\infty|\gamma(h)|&lt;\infty\)`.

&gt; * Se `\(\{X_t\}\)` é um processo estacionário gaussiano então 
`\(\sqrt{n}\left(\bar{X}_n-\mu\right)\)` é assintoticamente normal com
média `\(0\)` e variância `\(\sum_{h=-n}^n\left(1-\frac{|h|}{n}\right)\gamma(h)\)`.

&gt; * Um intervalo de confiança aproximado de `\(95\%\)` para `\(\mu\)` é dado por
`$$\left[\bar{X}_n-1.96\left(\frac{1}{n}\sum_{|h|&lt;\infty}\gamma(h)\right)^{1/2}; \bar{X}_n+1.96\left(\frac{1}{n}\sum_{|h|&lt;\infty}\gamma(h)\right)^{1/2}\right].$$`


---
class: animated, fadeIn

### Função de autocovariância amostral

* **Definição**: Seja `\(\{X_t\}\)` um processo estacionário de segunda 
ordem, o estimador da função de autocovariância `\(\gamma(\cdot)\)` e a função
de autocorrelação `\(\rho(\cdot)\)` são definidas como
`$$\hat{\gamma}(h)=\frac1n\sum_{t=1}^{n-|h|}\left(X_{t+|h|}-\bar{X}_n\right)\left(X_{t}-\bar{X}_n\right)$$`
e
`$$\hat{\rho}(h)=\frac{\hat{\gamma}(h)}{\hat{\gamma}(0)}.$$`

&gt; * O estimador de `\(\gamma(h)\)`, definido acima, é uma função par e não-negativa 
definida&lt;sup&gt;1&lt;/sup&gt;;

&gt; * Ambos os estimadores são viesados, mesmo quando o fator `\(\frac1n\)` é
substituido por `\(\frac{1}{n-h}\)`. No entanto, sob suposições gerais, os
estimadores são assintoticamente não-viesados para tamanhos de amostra 
suficientemente grande. 

***
&lt;font size=3&gt;&lt;sup&gt;1&lt;/sup&gt; Vide, Brockwell &amp; Davis (2016), p. 51.&lt;/font&gt;

---
class: animated, fadeIn

### Função de autocovariância amostral

&gt; Considere os seguintes estimadores para `\(\gamma(h)\)`:  `\(\hat{\gamma}(h)=\frac1n\sum_{t=1}^{n-|h|}\left(X_{t+|h|}-\bar{X}_n\right)\left(X_{t}-\bar{X}_n\right)\)` e `\(\hat{\gamma}^*(h)=\frac{1}{n-h}\sum_{t=1}^{n-|h|}\left(X_{t+|h|}-\bar{X}_n\right)\left(X_{t}-\bar{X}_n\right)\)`, então
&lt;div class="math"&gt;
  \begin{align*}
    \mathbb{E}\left[\hat{\gamma}(h)\right]&amp;\approx\gamma(h)-\frac{h}{n}\gamma(h)-\left(1-\frac{h}{n}\right)\mathrm{var}\left(\bar{X}_n\right)\\
    \mathbb{E}\left[\hat{\gamma}^*(h)\right]&amp;\approx\gamma(h)-\mathrm{var}\left(\bar{X}_n\right)
  \end{align*}
&lt;/div&gt;

&gt; Quando `\(\{X_t\}\)` é um processo gaussiano, Barttlet (1946) mostrou que
&lt;div class="math"&gt;
  \begin{align*}
    \mathrm{var}\left(\hat{\gamma}(h)\right)&amp;\approx\frac1n\sum_{i=-\infty}^\infty\left(\gamma^2(i)+\gamma(i+h)\gamma(i-h)\right)\\
    \mathrm{var}\left(\hat{\gamma}^*(h)\right)&amp;\approx\frac{1}{n-h}\sum_{i=-\infty}^\infty\left(\gamma^2(i)+\gamma(i+h)\gamma(i-h)\right)
  \end{align*}
&lt;/div&gt;

***
&lt;font size=3&gt;Barttlet, M. S. (1946). On the theoretical specification  of
sampling properties of autocorrelated time series. Journal of the royal 
statistical society, B8, 27-41.&lt;/font&gt;

---
class: animated, fadeInDown

### Função de autocovariância amostral

&gt; A covariância dos estimadores é dada por
&lt;div class="math"&gt;
  \begin{align*}
    \mathrm{Cov}\left(\hat{\gamma}(h),\hat{\gamma}(h+j)\right)&amp;\approx\frac1n\sum_{i=-\infty}^\infty\left(\gamma(i)\gamma(i+j)+\gamma(i+h+j)\gamma(i-h)\right)\\
    \mathrm{Cov}\left(\hat{\gamma}^*(h),\hat{\gamma}^*(h+j)\right)&amp;\approx\frac{1}{n-h}\sum_{i=-\infty}^\infty\left(\gamma(i)\gamma(i+j)+\gamma(i+h+j)\gamma(i-h)\right)
  \end{align*}
&lt;/div&gt;

No caso da função de autocorrelação, se o processo é gaussiano então
`\(\hat{\rho}(h)\)` é assintoticamente normal com média `\(\rho(h)\)` e
variância dada por
`$$\mathrm{var}\left(\hat{\rho}(h)\right)\approx\frac1n\sum_{i=-\infty}^\infty\left(\rho^2(i)+\rho(i+h)\rho(i-h)-4\rho(h)\rho(i)\rho(i-h)+2\rho^2(h)\rho^2(i)\right)$$`
Para processos tais que `\(\rho(h)=0\)`, para `\(h&gt;m\)`, a aproximação de 
Barttlet pode ser reduzida em
`$$\mathrm{var}\left(\hat{\rho}(h)\right)\approx\frac1n\left(1+2\rho^2(1)+2\rho^2(2)+\cdots+2\rho^2(m)\right).$$`


---
class: animated, fadeRight

### Função de autocorrelação parcial amostral

&gt; Na prática, os valores de `\(\rho(\cdot)\)` são desconhecidos e podem
ser substituidos pelos valores estimados, i.e.
`$$\mathrm{var}\left(\hat{\rho}(h)\right)\approx\frac1n\left(1+2\hat{\rho}^2(1)+2\hat{\rho}^2(2)+\cdots+2\hat{\rho}^2(m)\right).$$`

* **Definição (FAC parcial)**: Seja `\(\{X_t\}\)` um processo estacionário
de segunda ordem. A função de autocorrelação parcial define-se como
`$$\mathrm{Corr}(X_t,X_{t+h}|X_{t+1},X_{t+2},\ldots,X_{t+h-1}).$$`
 &gt; A FAC parcial, ou FACP, também pode ser definida como a função `\(\alpha(n)=\mathbb{I}_{\{0\}}(n)+\phi_{nn}\mathbb{I}_{\{2,3,\ldots,\}}(n)\)`,
 onde `\(\phi_{nn}\)` é obtido calculando o preditor linear usando o 
 **[algoritmo de Durbin-levinson](https://ffajardo64.github.io/statistical_learning/STA13828/aula4.html#16)**.

A FACP também pode ser calculada considerando o seguinte modelo de 
regressão
`$$X_{t+k}=\phi_{k1}X_{t+k-1}+\phi_{k2}X_{t+k-2}+\cdots+\phi_{kk}X_{t}+e_{t+k},$$`
onde `\(\phi_{ki}\)` denota o `\(i\)`-ésimo parámetro da regressão e `\(e_{t+k}\)` 
representa o termo de erro com média `\(0\)` e variância constante, sendo
ele não-correlacionado com `\(X_{t+k-j}\)` para `\(j=1,2,\ldots,k\)`.


---
class: animated, fadeDown

### Função de autocorrelação parcial amostral

Multiplicando por `\(X_{t+k-j}\)` e aplicando o operador valor esperado, 
temos que

&lt;div class="math"&gt;
  \begin{align*}
    \mathbb{E}[X_{t+k}X_{t+k-j}]&amp;=\phi_{k1}\mathbb{E}[X_{t+k-1}X_{t+k-j}]+\phi_{k2}\mathbb{E}[X_{t+k-2}X_{t+k-j}]+\cdots+\phi_{kk}\mathbb{E}[X_{t}X_{t+k-j}]+\mathbb{E}[e_{t+k}X_{t+k-j}]\\
    \gamma(j)&amp;=\phi_{k1}\gamma(j-1)+\phi_{k2}\gamma(j-2)+\cdots+\phi_{kk}\gamma(j-k)\\
  \end{align*}
&lt;/div&gt;

ou 

`$$\rho(j)=\phi_{k1}\rho(j-1)+\phi_{k2}\rho(j-2)+\cdots+\phi_{kk}\rho(j-k).$$`
Daí, para `\(j=1,2,\ldots,k\)` temos o seguinte sistema de equações

&lt;div class="math"&gt;
  \begin{align*}
    \rho(1)&amp;=\phi_{k1}\rho(0)+\phi_{k2}\rho(-1)+\cdots+\phi_{kk}\rho(k-1)\\
    \rho(2)&amp;=\phi_{k1}\rho(1)+\phi_{k2}\rho(0)+\cdots+\phi_{kk}\rho(k-2)\\
    &amp;\ \ \vdots\\
    \rho(k)&amp;=\phi_{k1}\rho(k-1)+\phi_{k2}\rho(k-2)+\cdots+\phi_{kk}\rho(0)
  \end{align*}
&lt;/div&gt;

---
class: animated, fadeLeft

### Função de autocorrelação parcial amostral
Usando a regra de Cramer, para `\(k=1,2,3,\)`
&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;

&lt;div class="math"&gt;
  \begin{align*}
    \phi_{11}&amp;=\rho(1) &amp; 
    \phi_{22}&amp;=\frac{\begin{vmatrix}
    1 &amp; \rho(1)\\
    \rho(1) &amp; \rho(2)
    \end{vmatrix}}{\begin{vmatrix}
    1 &amp; \rho(1)\\
    \rho(1) &amp; 1
    \end{vmatrix}}
    &amp; \phi_{33}&amp;=\frac{\begin{vmatrix}
    1 &amp; \rho(1) &amp; \rho(1)\\
    \rho(1) &amp; 1 &amp; \rho(2)\\
    \rho(2) &amp; \rho(1) &amp; \rho(3)
    \end{vmatrix}}{\begin{vmatrix}
    1 &amp; \rho(1) &amp; \rho(2)\\
    \rho(1) &amp; 1 &amp; \rho(1)\\
    \rho(2) &amp; \rho(1) &amp; 1
    \end{vmatrix}}
  \end{align*}
&lt;/div&gt;

---
class: animated, fadeIn

### Função de autocorrelação parcial amostral
&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;

&lt;div class="math"&gt;
  \begin{align*}
    \phi_{kk}&amp;=\frac{\begin{vmatrix}
    1 &amp; \rho(1) &amp; \rho(2) &amp; \cdots &amp; \rho(k-2) &amp; \rho(1)\\
    \rho(1) &amp; 1 &amp; \rho(1)&amp; \cdots &amp; \rho(k-3) &amp; \rho(2)\\
    \vdots &amp; &amp; &amp; &amp; &amp; \vdots\\
    \rho(k-1) &amp; \rho(k-2) &amp; \rho(k-3) &amp; \cdots &amp; \rho(1) &amp; \rho(k)
    \end{vmatrix}}{\begin{vmatrix}
    1 &amp; \rho(1) &amp; \rho(2) &amp; \cdots &amp; \rho(k-2) &amp; \rho(k-1)\\
    \rho(1) &amp; 1 &amp; \rho(1)&amp; \cdots &amp; \rho(k-3) &amp; \rho(k-2)\\
    \vdots &amp; &amp; &amp; &amp; &amp; \vdots\\
    \rho(k-1) &amp; \rho(k-2) &amp; \rho(k-3) &amp; \cdots &amp; \rho(1) &amp; 1
    \end{vmatrix}}
  \end{align*}
&lt;/div&gt;


---
class: animated, fadeUp

### Função de autocorrelação parcial amostral

Na prática, os valores da FACP são calculados usando algoritmos 
iterativos, iniciando com `\(\hat{\phi}_{11}=\hat{\rho}(1)\)` e 
`$$\hat{\phi}_{k+1,k+1}=\frac{\hat{\rho}(k+1)-\sum_{j=1}^k\hat{\phi}_{kj}\hat{\rho}_{k+1-j}}{1-\sum_{j=1}^k\hat{\phi}_{kj}\hat{\rho}_j}$$`
e `\(\hat{\phi}_{k+1,j}=\hat{\phi}_{kj}-\hat{\phi}_{k+1,k+1}\,\hat{\phi}_{k,k+1-j}\)`,
para `\(=1,2,\ldots,k\)`.

Assumindo que o processo é ruído branco, a variância de `\(\hat{\phi}_{kk}\)` é
aproximadamente `\(\mathrm{var}\left[\hat{\phi}_{kk}\right]\approx\frac1n\)`.

&gt; * A identificação das ordens de processos com representação de médias 
móveis pode ser realizada com a função de autocorrelação amostral;

&gt; * Sendo um processo com representação autorregressiva, a identificação da ordem
do processo é praticamente impossível só com o usao da função de 
autocorrelação amostral. Nesse cenário, a função de autocorrelação 
parcial amostral resulta mais adequada para a identificação da ordem 
autorregressiva.


---
class: animated, fadeIn

### Função de autocorrelação estendida amostral

---
class: animated, fadeIn

### Função de autocorrelação inversa amostral



---
class: animated, lightSpeedIn
### Referências

Box, G. and G. M. Jenkins (1976). _Time Series Analysis: Forecasting and Control_.
Holden-Day.

Brockwell, P. and R. Davis (2016). _Introduction to Time Series and Forecasting_. 3rd.
Springer Verlag.

Hamilton, J. M. (1994). _Time Series Analysis_. Princeton University Press.

Wei, W. (2005). _Time Series Analysis: Univariate and Multivariate Methods_. 2nd. Addison
Wesley.

---
class: animated, hide-logo, bounceInDown
## Política de proteção aos direitos autorais

&gt; &lt;span style="color:grey"&gt;O conteúdo disponível consiste em material protegido pela legislação brasileira, sendo certo que, por ser
o detentor dos direitos sobre o conteúdo disponível na plataforma, o **LECON** e o **NEAEST** detém direito
exclusivo de usar, fruir e dispor de sua obra, conforme Artigo 5&lt;sup&gt;o&lt;/sup&gt;, inciso XXVII, da Constituição Federal
e os Artigos 7&lt;sup&gt;o&lt;/sup&gt; e 28&lt;sup&gt;o&lt;/sup&gt;, da Lei 9.610/98.
A divulgação e/ou veiculação do conteúdo em sites diferentes à plataforma e sem a devida autorização do
**LECON** e o **NEAEST**, pode configurar violação de direito autoral, nos termos da Lei 9.610/98, inclusive podendo
caracterizar conduta criminosa, conforme Artigo 184&lt;sup&gt;o&lt;/sup&gt;, §1&lt;sup&gt;o&lt;/sup&gt; a 3&lt;sup&gt;o&lt;/sup&gt;, do Código Penal.
É considerada como contrafação a reprodução não autorizada, integral ou parcial, de todo e qualquer
conteúdo disponível na plataforma.&lt;/span&gt;

.pull-left[
&lt;img src="images/logo_lecon.png" width="50%" style="display: block; margin: auto;" /&gt;
]
.pull-right[
&lt;img src="images/logo_neaest.png" width="50%" style="display: block; margin: auto;" /&gt;
]
&lt;br&gt;&lt;/br&gt;
.center[
[https://lecon.ufes.br](https://lecon.ufes.br/) &amp;emsp; &amp;emsp;  &amp;emsp; &amp;emsp; [https://analytics.ufes.br](https://analytics.ufes.br)
]

&lt;font size="2"&gt;&lt;span style="color:grey"&gt;Material elaborado pela equipe LECON/NEAEST: 
Alessandro J. Q. Sarnaglia, Bartolomeu Zamprogno, Fabio A. Fajardo, Luciana G. de Godoi 
e Nátaly A. Jiménez.&lt;/span&gt;&lt;/font&gt;
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>
<style>
.logo {
  background-image: url(images/logo_neaest.png);
  background-size: contain;
  background-repeat: no-repeat;
  position: absolute;
  top: 1em;
  right: 1em;
  width: 150px;
  height: 168px;
  z-index: 0;
}
</style>

<script>
document
  .querySelectorAll(
    '.remark-slide-content' +
    ':not(.title-slide)' +
    // add additional classes to exclude here, e.g.
    // ':not(.inverse)' +
    ':not(.hide-logo)'
  )
  .forEach(el => {
    el.innerHTML += '<div class="logo"></div>';
  });
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
