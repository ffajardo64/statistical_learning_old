---
title: "STA13824 - Análise de regressão"
subtitle: "Caso particular: Modelo de regressão linear simples"
author: "<br/><br/>"
institute: "LECON/DEST - UFES"
date: "Vitória, ES. - `r format(Sys.Date(), format='%d/%m/%Y')`"
output:
  xaringan::moon_reader:
    includes:
      after_body: insert-logo.html
    css: [xaringan-themer.css, "https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.7.0/animate.min.css"]
    lib_dir: libs
    nature:
      ratio: '16:9' 
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

class: animated, fadeIn
```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer); library(dplyr); library("DT"); library(plotly) #library(icons)
style_mono_light(base_color = "#23395b")
```
```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```
```{r, load_refs, include=FALSE, cache=FALSE}
library("RefManageR"); library("bibtex")
BibOptions(check.entries = FALSE,
           bib.style = "authoryear",
           cite.style = "authoryear",
           style = "markdown",
           hyperlink = FALSE,
           dashed = FALSE)
myBib <- ReadBib("ref_ementa.bib", check = FALSE)
```

<style> body {text-align: justify} </style> <!-- Justify text. -->

### Alguns resultados da álgebra matricial - I
* **Particição de matrizes**: Uma matriz $\mathbf{A}$ pode ser subdividida em 
blocos, ou particionada em matrizes menores, inserindo cortes horizontais e 
verticais entre linhas e colunas. Dessa forma, a matriz $\mathbf{A}$ pode ser
representada com

<div class="math">
\[\mathbf{A}=
\left[
\begin{array}{cc}
\mathbf{A}_{11}& \mathbf{A}_{12} \\
\mathbf{A}_{21} & \mathbf{A}_{22}
\end{array}
\right].
\]
</div>

> Por conveniência, assumiremos que a dimensão do $\mathbf{A}_{11}$ é $n\times n$ e a dimensão do
$\mathbf{A}_{22}$ é $m\times m$.

#### Propriedades - I

* **Trasposta**:

<div class="math">
\[\mathbf{A}'=
\left[
\begin{array}{cc}
\mathbf{A}'_{11}& \mathbf{A}'_{21} \\
\mathbf{A}'_{12} & \mathbf{A}'_{22}
\end{array}
\right].
\]
</div>

* **Determinante**:

<div class="math">
\[\det\mathbf{A}=\left|\textbf{A}\right|=
\left|
\begin{array}{cc}
\mathbf{A}_{11}& \mathbf{A}_{12} \\
\mathbf{A}_{21} & \mathbf{A}_{22}
\end{array}
\right|=\left|\textbf{A}_{11}\right|\left|\textbf{A}_{22}-\textbf{A}_{21}\textbf{A}_{11}^{-1}\textbf{A}_{12}\right|=\left|\textbf{A}_{22}\right|\left|\textbf{A}_{11}-\textbf{A}_{12}\textbf{A}_{22}^{-1}\textbf{A}_{21}\right|.
\]
</div>

---
class: animated, slideInRight

### Alguns resultados da álgebra matricial - II

#### Propriedades - II

* **Inversa**:

<div class="math">
\[ A^{-1}=
\left[ 
\begin{array}{cc}
\mathbf{A}_{11}&\mathbf{A}_{12}\\
\mathbf{A}_{21}&\mathbf{A}_{22}
\end{array} \right]^{-1}
=
\left[ 
\begin{array}{cc}
\mathbf{A}_{11}^{-1}+\mathbf{A}_{11}^{-1}\mathbf{A}_{12}\mathbf{Q}^{-1}\mathbf{A}_{21}\mathbf{A}_{11}^{-1}&-\mathbf{A}_{11}^{-1}\mathbf{A}_{12}\mathbf{Q}^{-1}\\
-\textbf{Q}^{-1}\mathbf{A}_{21}\mathbf{A}_{11}^{-1}&\mathbf{Q}^{-1}
\end{array} \right],
\]
</div>

onde a matriz $\mathbf{Q}=\mathbf{A}_{22}-\mathbf{A}_{21}\mathbf{A}_{11}^{-1}\mathbf{A}_{12}$
é chamada de *complemento de Schur*. A inversa da matriz $\textbf{A}$ existe see
o *complemento de Schur* é não-singular. Para detalhes ver, por exemplo, o livro de `r Citet(myBib, "greene2003")`. 

* **Adição**:

<div class="math">
\[\mathbf{A}+\mathbf{B}=
\left[
\begin{array}{cc}
\mathbf{A}_{11}& \mathbf{A}_{12} \\
\mathbf{A}_{21} & \mathbf{A}_{22}
\end{array}
\right]+
\left[
\begin{array}{cc}
\mathbf{B}_{11}& \mathbf{B}_{12} \\
\mathbf{B}_{21} & \mathbf{B}_{22}
\end{array}
\right]=
\left[
\begin{array}{cc}
\mathbf{A}_{11}+\mathbf{B}_{11}& \mathbf{A}_{12}+\mathbf{B}_{12} \\
\mathbf{A}_{21}+\mathbf{B}_{21} & \mathbf{A}_{22}+\mathbf{B}_{22}
\end{array}
\right].
\]
</div>

* **Produto**:

<div class="math">
\[\mathbf{A}\mathbf{B}=
\left[
\begin{array}{cc}
\mathbf{A}_{11}\mathbf{B}_{11}+\mathbf{A}_{12}\mathbf{B}_{21}& \mathbf{A}_{11}\mathbf{B}_{12}+\mathbf{A}_{12}\mathbf{B}_{22}  \\
\mathbf{A}_{21}\mathbf{B}_{11}+\mathbf{A}_{22}\mathbf{B}_{21} & \mathbf{A}_{21}\mathbf{B}_{12}+\mathbf{A}_{22}\mathbf{B}_{22}
\end{array}
\right].
\]
</div>

---
class: animated, slideInRight

### Modelo de regressão linear - I

Considere o modelo é linear, dado por

$$Y=\mathbf{X\,\beta}+\mathbf{\epsilon}.$$
Pelas Suposições **A1** e **A2**, o estimador de MQO para $\mathbf{\beta}$ é 
dado por

$$\mathbf{\widehat{\beta}}=\left(\mathbf{X}'\mathbf{X}\right)^{-1}\mathbf{X}'Y.$$
> Pelas Suposições **A1-A3**, o estimador MQO é não-viesado para $\mathbf{\beta}$, i.e.,
$\mathbb{E}\left[\widehat{\mathbf{\beta}}\,\big|\,\mathbf{X}\right]=\mathbf{\beta}.$

> Pelas Suposições **A1-A4**, $\textrm{cov}\left[\widehat{\beta}\,\big|\,\mathbf{X}\right]=\sigma^2\left(\mathbf{X}'\mathbf{X}\right)^{-1}$.

Considere o caso particular quando $k=1$. Nesse caso, a matrix de covariáveis é dada por:

<div class="math">
\[\textbf{X}=
\left[
\begin{array}{cc}
1&x_{1}\\
1&x_{2}\\
\vdots&\vdots\\
1&x_{n}\\
\end{array}
\right]_{n\times 2}=[\mathbf{1} \quad \mathbf{x}_1].
\]
</div>

---
class: animated, slideInRight

### Modelo de regressão linear - II

A representação matricial do modelo linear simples, i.e., quando $k=1$, é dada por

<div class="math">
\[
\left[
\begin{array}{c}
Y_1 \\
Y_2 \\
\vdots\\
Y_n
\end{array}
\right]=
\left[
\begin{array}{cc}
1&x_{1}\\
1&x_{2}\\
\vdots&\vdots\\
1&x_{n}\\
\end{array}
\right]
\left[
\begin{array}{c}
\mathbf{\beta}_0 \\
\mathbf{\beta}_1 \\
\end{array}
\right]+
\left[
\begin{array}{c}
\mathbf{\epsilon}_1 \\
\mathbf{\epsilon}_2 \\
\vdots\\
\mathbf{\epsilon}_n
\end{array}
\right]=[\mathbf{1} \quad \mathbf{x}_1]
\left[
\begin{array}{c}
\mathbf{\beta}_0 \\
\mathbf{\beta}_1 \\
\end{array}
\right]+\mathbf{\epsilon}
\]
</div>

ou

$$Y = \beta_0\mathbf{1}+\mathbf{\beta}_1\mathbf{x}_1+\mathbf{\epsilon}.$$
Pelas Suposições **A1** e **A2**, o estimador de MQO para $\mathbf{\beta}$ é 
dado por

$$\mathbf{\widehat{\beta}}=\left(\mathbf{X}'\mathbf{X}\right)^{-1}\mathbf{X}'Y$$
ou

<div class="math">
\[\widehat{\mathbf{\beta}}=
\left[
\begin{array}{cc}
\mathbf{1}'\mathbf{1} & \mathbf{1}'\mathbf{x}_1  \\
\mathbf{x}'_1\mathbf{1} & \mathbf{x}'_1\mathbf{x}_1 \\
\end{array}
\right]^{-1}
\left[
\begin{array}{c}
\mathbf{1}'Y\\
\mathbf{x}'_1Y
\end{array}
\right]=
\left[
\begin{array}{cc}
n & \sum_{i=1}^nx_i  \\
\sum_{i=1}^nx_i & \sum_{i=1}^nx_i^2 \\
\end{array}
\right]^{-1}
\left[
\begin{array}{c}
\sum_{i=1}^nY_i\\
\sum_{i=1}^nx_iY_i
\end{array}
\right]
\]
</div>


---
class: animated, fadeIn

### Modelo de regressão linear - II

<div class="math">
\[
\begin{aligned}
\widehat{\mathbf{\beta}}&=
\left[
\begin{array}{c}
\widehat{\beta}_0\\
\widehat{\beta}_1
\end{array}\right]=
\left[
\begin{array}{cc}
n & \sum_{i=1}^nx_i  \\
\sum_{i=1}^nx_i & \sum_{i=1}^nx_i^2 \\
\end{array}
\right]^{-1}
\left[
\begin{array}{c}
\sum_{i=1}^nY_i\\
\sum_{i=1}^nx_iY_i
\end{array}
\right]\\
&=\frac{1}{n\sum_{i=1}^nx_i^2-\left(\sum_{i=1}x_i\right)^2}
\left[
\begin{array}{cc}
\sum_{i=1}^nx_i^2 & -\sum_{i=1}^nx_i  \\
-\sum_{i=1}^nx_i & n \\
\end{array}
\right]
\left[
\begin{array}{c}
\sum_{i=1}^nY_i\\
\sum_{i=1}^nx_iY_i
\end{array}
\right]\\
&=\frac{1}{n\sum_{i=1}^nx_i^2-\left(\sum_{i=1}x_i\right)^2}
\left[
\begin{array}{c}
\sum_{i=1}^nx_i^2\sum_{i=1}^nY_i-\sum_{i=1}^nx_i\sum_{i=1}^nx_iY_i\\
-\sum_{i=1}^nx_i\sum_{i=1}^nY_i+n\sum_{i=1}^nx_iY_i
\end{array}
\right]
\end{aligned}
\]
</div>

Daí,

<div class="math">
\[
\begin{aligned}
\widehat{\mathbf{\beta}}_1&=
\frac{n\sum_{i=1}^nx_iY_i-\sum_{i=1}^nx_i\sum_{i=1}^nY_i}{n\sum_{i=1}^nx_i^2-\left(\sum_{i=1}x_i\right)^2}=
\frac{n\sum_{i=1}^n\left(x_i-\bar{x}\right)(Y_i-\bar{Y})}{n\sum_{i=1}^n\left(x_i-\bar{x}\right)^2}
=\frac{\sum_{i=1}^n\left(x_i-\bar{x}\right)(Y_i-\bar{Y})}{\sum_{i=1}^n\left(x_i-\bar{x}\right)^2}.\\ \\

\widehat{\mathbf{\beta}}_0&=
\frac{\sum_{i=1}^nx_i^2\sum_{i=1}^nY_i-\sum_{i=1}^nx_i\sum_{i=1}^nx_iY_i}{n\sum_{i=1}^n\left(x_i-\bar{x}\right)^2}
=\frac{\sum_{i=1}^nx_i^2\bar{Y}-\bar{x}\sum_{i=1}^nx_iY_i}{\sum_{i=1}^n\left(x_i-\bar{x}\right)^2}
\end{aligned}
\]
</div>

---
class: animated, fadeIn

### Modelo de regressão linear - II

Observe que

<div class="math">
\[
\begin{aligned}
\widehat{\mathbf{\beta}}_0&=
\frac{\sum_{i=1}^nx_i^2\bar{Y}-\bar{x}\sum_{i=1}^nx_iY_i}{\sum_{i=1}^n\left(x_i-\bar{x}\right)^2}
=\frac{\sum_{i=1}^nx_i^2\bar{Y}-\bar{x}\sum_{i=1}^nx_iY_i+n\bar{x}^2\bar{Y}-n\bar{x}^2\bar{Y}}{\sum_{i=1}^n\left(x_i-\bar{x}\right)^2}\\
&=\frac{\sum_{i=1}^n\left(x_i-\bar{x}\right)^2\bar{Y}-\bar{x}\sum_{i=1}^n\left(x_i-\bar{x}\right)(Y_i-\bar{Y})}{\sum_{i=1}^n\left(x_i-\bar{x}\right)^2}\\
&=\bar{Y}-\widehat{\beta}_1\bar{x}.
\end{aligned}
\]
</div>

Dessa forma, os estimador de MQO para $\beta_0$ e $\beta_1$ são dados, 
respectivamente,  por

<div class="math">
\[
\widehat{\mathbf{\beta}}_0=\bar{Y}-\widehat{\beta}_1\bar{x} 
\qquad
\text{ e }
\qquad
\widehat{\mathbf{\beta}}_1=
\frac{\sum_{i=1}^n\left(x_i-\bar{x}\right)(Y_i-\bar{Y})}{\sum_{i=1}^n\left(x_i-\bar{x}\right)^2}.
\]
</div>

---
class: inverse, hide-logo, middle, center

# Teorema de Gauss-Markov

---
class: animated, slideInRight

### Teorema de Gauss-Markov - I

Considere o modelo é linear, dado por

$$Y=\mathbf{X\,\beta}+\mathbf{\epsilon}.$$

O teorema de Gauss-Markov afirma que, dadas as Suposições **A1-A4**, o estimador 
de MQO do vetor $\beta$ é o melhor estimador linear não-viesado e de mínima variância.


**Prova**

Defina a classe de estimadores lineares de $\beta$ como 
$\beta^*=\left[\left(\mathbf{X}'\mathbf{X}\right)^{-1}\mathbf{X}'+\mathbf{C}\right]Y$, onde 
$\mathbf{C}$ é uma matriz determinística de dimensão $(k+1)\times n$.

Note que o estimador de MQO, $\widehat{\beta}$, é um membro desta classe quando 
$\mathbf{C}$ é a matriz nula. Observe ainda que

<div class="math">
\[
\begin{aligned}
\beta^*&=\left[\left(\mathbf{X}'\mathbf{X}\right)^{-1}\mathbf{X}'+\mathbf{C}\right]Y=
\left[\left(\mathbf{X}'\mathbf{X}\right)^{-1}\mathbf{X}'+\mathbf{C}\right](\mathbf{X\beta}+\epsilon)\\
&=\left(\mathbf{X}'\mathbf{X}\right)^{-1}\mathbf{X}'\mathbf{X\beta}+\mathbf{C}\mathbf{X\beta}+\left(\mathbf{X}'\mathbf{X}\right)^{-1}\mathbf{X}'\epsilon+\mathbf{C}\epsilon\\
&=\beta+\mathbf{C}\mathbf{X\beta}+\left(\mathbf{X}'\mathbf{X}\right)^{-1}\mathbf{X}'\epsilon+\mathbf{C}\epsilon\\
\end{aligned}
\]
</div>

---
class: animated, fadeIn

### Teorema de Gauss-Markov - II

Dessa forma, 


<div class="math">
\[
\begin{aligned}
\beta^*&=\beta+\mathbf{C}\mathbf{X\beta}+\left(\mathbf{X}'\mathbf{X}\right)^{-1}\mathbf{X}'\epsilon+\mathbf{C}\epsilon\\
\end{aligned}
\]
</div>

Calculando o valor esperado, temos que

$$\mathbb{E}\beta^*=\beta+\mathbf{C}\mathbf{X\beta}=\left(\mathbf{I}+\mathbf{CX}\right)\beta.$$
Assim, para que $\beta^*$ seja não-viesado requer-se que $\mathbf{CX}=0$.

Temos então, 

$$\beta^*-\beta=\left(\mathbf{X}'\mathbf{X}\right)^{-1}\mathbf{X}'\epsilon+\mathbf{C}\epsilon
=\left[\left(\mathbf{X}'\mathbf{X}\right)^{-1}\mathbf{X}'+\mathbf{C}\right]\epsilon$$

e portanto

<div class="math">
\[
\begin{aligned}
\left(\beta^*-\beta\right)\left(\beta^*-\beta\right)'
&=\left[\left(\mathbf{X}'\mathbf{X}\right)^{-1}\mathbf{X}'+\mathbf{C}\right]\epsilon\epsilon'\left[\left(\mathbf{X}'\mathbf{X}\right)^{-1}\mathbf{X}'+\mathbf{C}\right]'
=\left[\left(\mathbf{X}'\mathbf{X}\right)^{-1}\mathbf{X}'+\mathbf{C}\right]\epsilon\epsilon'\left[\mathbf{X}\left(\mathbf{X}'\mathbf{X}\right)^{-1}+\mathbf{C}'\right]
\end{aligned}
\]
</div>

---
class: animated, fadeIn

### Teorema de Gauss-Markov - III

A matriz de covariâncias de $\beta^*$ é dada por

<div class="math">
\[
\begin{aligned}
\textrm{cov}\left[\beta^*\right]&=\mathbb{E}\left[\left(\beta^*-\beta\right)\left(\beta^*-\beta\right)'\right]
=\left[\left(\mathbf{X}'\mathbf{X}\right)^{-1}\mathbf{X}'+\mathbf{C}\right]\mathbb{E}\left[\epsilon\epsilon'\right]\left[\left(\mathbf{X}'\mathbf{X}\right)^{-1}\mathbf{X}'+\mathbf{C}\right]'\\
&=\left[\left(\mathbf{X}'\mathbf{X}\right)^{-1}\mathbf{X}'+\mathbf{C}\right]\sigma^2\mathbf{I}\left[\mathbf{X}\left(\mathbf{X}'\mathbf{X}\right)^{-1}+\mathbf{C}'\right]\\
&=\sigma^2\left[\left(\mathbf{X}'\mathbf{X}\right)^{-1}\mathbf{X}'\mathbf{X}\left(\mathbf{X}'\mathbf{X}\right)^{-1}+\mathbf{C}\mathbf{X}\left(\mathbf{X}'\mathbf{X}\right)^{-1}+\left(\mathbf{X}'\mathbf{X}\right)^{-1}\mathbf{X}'\mathbf{C}'+\mathbf{C}\mathbf{C}'\right]\\
&=\sigma^2\left[\left(\mathbf{X}'\mathbf{X}\right)^{-1}+\mathbf{C}\mathbf{C}'\right]=\sigma^2\left(\mathbf{X}'\mathbf{X}\right)^{-1}+\sigma^2\mathbf{C}\mathbf{C}'=\textrm{cov}\left[\widehat{\beta}\right]+\sigma^2\mathbf{C}\mathbf{C}'.
\end{aligned}
\]
</div>

As variâncias são os elementos diagonais da matriz de covariâncias. Dessa forma,
$\textrm{var}(\beta_i^*)=\textrm{var}(\widehat{\beta}_i)+c_i$, onde $c_i$ é 
não-negativo para $i=1,2,3,\ldots, k+1$. Assim, 

$$\textrm{var}(\beta_i^*)\ge\textrm{var}(\widehat{\beta}_i),$$
onde a igualdade se tem quando $\mathbf{C}$ é a matriz nula, i.e. $\beta^*=\widehat{\beta}$.


Para detalhes ver, por exemplo, o livro de `r Citet(myBib, "montgomery2006")`.

---
class: animated, lightSpeedIn
# Referências

```{r refs, echo=FALSE, results="asis"}
PrintBibliography(myBib, .opts = list(check.entries = FALSE))
```

---
class: animated, hide-logo, bounceInDown
## Política de proteção aos direitos autorais

> <span style="color:grey">O conteúdo disponível consiste em material protegido pela legislação brasileira, sendo certo que, por ser
o detentor dos direitos sobre o conteúdo disponível na plataforma, o **LECON** e o **NEAEST** detém direito
exclusivo de usar, fruir e dispor de sua obra, conforme Artigo 5<sup>o</sup>, inciso XXVII, da Constituição Federal
e os Artigos 7<sup>o</sup> e 28<sup>o</sup>, da Lei 9.610/98.
A divulgação e/ou veiculação do conteúdo em sites diferentes à plataforma e sem a devida autorização do
**LECON** e o **NEAEST**, pode configurar violação de direito autoral, nos termos da Lei 9.610/98, inclusive podendo
caracterizar conduta criminosa, conforme Artigo 184<sup>o</sup>, §1<sup>o</sup> a 3<sup>o</sup>, do Código Penal.
É considerada como contrafação a reprodução não autorizada, integral ou parcial, de todo e qualquer
conteúdo disponível na plataforma.</span>

.pull-left[
```{r, out.width='50%', fig.align='center', fig.cap='',echo=FALSE}
knitr::include_graphics("images/logo_lecon.png")
```
]
.pull-right[
```{r, out.width='50%', fig.align='center', fig.cap='',echo=FALSE}
knitr::include_graphics("images/logo_neaest.png")
```
]
<br></br>
.center[
[https://lecon.ufes.br](https://lecon.ufes.br/) &emsp; &emsp;  &emsp; &emsp; [https://analytics.ufes.br](https://analytics.ufes.br)
]

<font size="2"><span style="color:grey">Material elaborado pela equipe LECON/NEAEST: 
Alessandro J. Q. Sarnaglia, Bartolomeu Zamprogno, Fabio A. Fajardo, Luciana G. de Godoi 
e Nátaly A. Jiménez.</span></font>
